{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969S__pMmA4o"
      },
      "source": [
        "# Prompt Engineering Activity\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "Before starting, make sure you have:\n",
        "- **HuggingFace Token**: Get from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "- **Google Colab Account** (Free tier works!) - *Optional, can run locally too*\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Quick Links\n",
        "\n",
        "**üîó Open in Colab**: [Click here](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering_Basics.ipynb)\n",
        "\n",
        "**üìÇ View on GitHub**: [Click here](https://github.com/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering_Basics.ipynb)\n",
        "\n",
        "> **‚ö†Ô∏è Note**: Colab link requires the repository to be public on GitHub. If you get a 404 error, see troubleshooting below.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Setup Instructions\n",
        "\n",
        "### Option 1: Google Colab (Recommended for GPU)\n",
        "\n",
        "#### Step 1: Open Notebook\n",
        "- **Method A**: Click the \"Open in Colab\" link above\n",
        "- **Method B**:\n",
        "  1. Go to [Google Colab](https://colab.research.google.com/)\n",
        "  2. Click **File** ‚Üí **Open notebook** ‚Üí **GitHub** tab\n",
        "  3. Enter: `oviya-raja/ist-402-assignments`\n",
        "  4. Navigate to: `IST402/assignments/W3/reference/W3__Prompt_Engineering_Basics.ipynb`\n",
        "\n",
        "> **‚ÑπÔ∏è Security Warning**: When opening from GitHub, Colab will show a warning dialog: *\"This notebook was not authored by Google\"*. This is **normal and safe** - it's Colab's security feature for external notebooks. Simply click **\"Run anyway\"** to proceed. The notebook is safe to use.\n",
        "\n",
        "#### Step 2: Enable GPU (Recommended)\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **GPU** ‚Üí **Save**\n",
        "3. **Runtime** ‚Üí **Restart runtime**\n",
        "\n",
        "#### Step 3: Set Up Token (See Token Setup section below)\n",
        "\n",
        "---\n",
        "\n",
        "### Option 2: Local Environment\n",
        "\n",
        "1. **Install dependencies**: Run Cell 2 (Install packages)\n",
        "2. **Set up token**: See Token Setup section below\n",
        "3. **Run cells in order**: Start from Cell 1\n",
        "\n",
        "---\n",
        "\n",
        "## üîê Token Setup\n",
        "\n",
        "### For Google Colab Users\n",
        "\n",
        "**Recommended Method: Using .env file**\n",
        "1. Run **Cell 4** ‚Üí It will automatically create a `.env` file\n",
        "2. Click the **folder icon (üìÅ)** in the left sidebar\n",
        "3. Find and click `.env` file\n",
        "4. Replace `your_token_here` with your actual token\n",
        "5. Save (Ctrl+S or Cmd+S)\n",
        "6. Re-run **Cell 4** ‚Üí Token loaded! ‚úÖ\n",
        "\n",
        "**Quick Method: Direct environment variable**\n",
        "Run this in a new cell before Cell 4:\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_actual_token_here\"\n",
        "```\n",
        "\n",
        "### For Local Users\n",
        "\n",
        "**Create `.env` file manually:**\n",
        "1. Create a file named `.env` in the same directory as this notebook\n",
        "2. Add this line: `HUGGINGFACE_HUB_TOKEN=your_actual_token_here`\n",
        "3. No spaces around the `=` sign!\n",
        "4. Run **Cell 4** ‚Üí Token loaded! ‚úÖ\n",
        "\n",
        "**Get your token**: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "> **üìñ Learn more**: See `ENV_IN_COLAB.md` for detailed explanation of how `.env` files work in Colab\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Troubleshooting\n",
        "\n",
        "### 404 Error When Opening from GitHub\n",
        "\n",
        "**Possible causes:**\n",
        "- Repository doesn't exist yet ‚Üí Use **Option 2** (Upload to Colab)\n",
        "- Repository is private ‚Üí Make it public or use **Option 2**\n",
        "- Wrong branch ‚Üí Try changing `main` to `master` in the link\n",
        "\n",
        "**Solution**: Upload the notebook directly to Colab:\n",
        "1. Download this notebook\n",
        "2. Go to [Google Colab](https://colab.research.google.com/)\n",
        "3. Click **File** ‚Üí **Upload notebook**\n",
        "4. Select the downloaded file\n",
        "\n",
        "### GPU Not Detected in Colab\n",
        "\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **GPU** ‚Üí **Save**\n",
        "3. **Runtime** ‚Üí **Restart runtime**\n",
        "4. Re-run Cell 1 to verify\n",
        "\n",
        "### Token Not Loading\n",
        "\n",
        "- Check that `.env` file exists and has correct format: `HUGGINGFACE_HUB_TOKEN=token` (no spaces)\n",
        "- Make sure you re-ran Cell 4 after creating/editing `.env`\n",
        "- Verify token is valid at [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ñ∂Ô∏è Getting Started\n",
        "\n",
        "1. **Run Cell 1**: Verify environment setup\n",
        "2. **Run Cell 2**: Install required packages\n",
        "3. **Run Cell 3**: Set up FAISS (CPU/GPU)\n",
        "4. **Run Cell 4**: Set up Hugging Face token\n",
        "5. **Continue**: Run remaining cells in order\n",
        "\n",
        "**Happy coding! üéâ**\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ About This Notebook\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "This notebook provides **fundamental prompt engineering exercises** using Mistral-7B-Instruct. It covers:\n",
        "\n",
        "- **Basic Prompt Engineering**: Learn how to create effective system prompts and user messages\n",
        "- **Model Interaction**: Understand how to use Hugging Face Transformers with Mistral-7B\n",
        "- **Pipeline vs Direct Model Loading**: Compare different approaches to using language models\n",
        "- **Device Optimization**: Automatic CPU/GPU detection and configuration\n",
        "- **Class Exercises**: Step-by-step exercises for creating business-specific AI assistants\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- **üìò [W3__Prompt_Engineering w_QA Applications-2.ipynb](https://github.com/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)**: Advanced RAG (Retrieval-Augmented Generation) system with FAISS vector database and QA model comparison\n",
        "  - [Open in Colab](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)\n",
        "\n",
        "### Learning Path\n",
        "\n",
        "1. **Start here** ‚Üí Learn prompt engineering basics with this notebook\n",
        "2. **Then** ‚Üí Move to the QA Applications notebook for RAG system implementation\n",
        "3. **Finally** ‚Üí Complete the RAG Assignment for full system design\n",
        "\n",
        "---\n",
        "\n",
        "**üìö This notebook is part of the IST402 - AI Agents & RAG Systems course materials.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ASvb8vxen1",
        "outputId": "7853d86c-7ba5-41b7-eb1e-4209f6cf4948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking Google Colab environment...\n",
            "   Python version: 3.12.12\n",
            "   ‚úÖ Running in Google Colab\n",
            "   ‚úÖ GPU Available: NVIDIA A100-SXM4-80GB\n",
            "   ‚úÖ CUDA Version: 12.6\n",
            "\n",
            "üìã Next Steps:\n",
            "   1. If GPU not detected in Colab: Enable GPU runtime and restart\n",
            "   2. Run Cell 2: Install packages\n",
            "   3. Run Cell 3: Set up Hugging Face token\n",
            "   4. Continue with remaining cells\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Setup Verification\n",
        "# Run this cell FIRST to check if everything is set up correctly\n",
        "\n",
        "import sys\n",
        "print(\"üîç Checking Google Colab environment...\")\n",
        "print(f\"   Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"   ‚úÖ Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"   ‚ö†Ô∏è  Not running in Google Colab (local environment)\")\n",
        "\n",
        "# Check GPU availability\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  GPU NOT detected\")\n",
        "        if IN_COLAB:\n",
        "            print(\"   üí° TIP: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
        "            print(\"   üí° Then: Runtime ‚Üí Restart runtime\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  PyTorch not installed yet (will be installed in next cell)\")\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"   1. If GPU not detected in Colab: Enable GPU runtime and restart\")\n",
        "print(\"   2. Run Cell 2: Install packages\")\n",
        "print(\"   3. Run Cell 3: Set up Hugging Face token\")\n",
        "print(\"   4. Continue with remaining cells\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q16cFgD3MfHJ",
        "outputId": "bdb0eb60-20e2-4a76-b739-a5cddeaa1983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages - run this cell first\n",
        "# Note: FAISS package will be installed conditionally based on GPU availability in Cell 3\n",
        "\n",
        "# Core packages (always needed)\n",
        "%pip install transformers torch sentence-transformers datasets python-dotenv\n",
        "\n",
        "# FAISS will be installed conditionally in Cell 3 based on device (CPU/GPU)\n",
        "%pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G-MQ5uhON96Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238bb5f6-2c12-445f-e11c-35cc42474650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Hugging Face token loaded successfully!\n",
            "   Token preview: hf_ThdSIol...ustv\n"
          ]
        }
      ],
      "source": [
        "# This cell automatically handles both Colab and local environments\n",
        "\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
        "\n",
        "print(\"‚úÖ Hugging Face token loaded successfully!\")\n",
        "print(f\"   Token preview: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else '****'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXMPHypoVUUS",
        "outputId": "d9e1b5bc-673b-4d28-a473-19c02f693a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries we need\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYOyg2_clJAL",
        "outputId": "760873a5-f3cd-4be5-8fcc-1b7439d443c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Automatically detect and configure device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "213bd56f7fec433bb552a57c0d19c9c9",
            "48eb2041e1e64e2299d9fa262b9e2b44",
            "a34839af6d964881ab34ddd6e8e314a0",
            "7d72bf576c554ee39cdc38f619c6711b",
            "c45108f07d4b4105a0a31a0ac0da00d6",
            "3c592cbd84d34e5d85645ed55013ac9b",
            "aec94d73cc3747edabc03453dc0ba15f",
            "1056aa7715fb4f5bba250e5513d8fad8",
            "e5766e803b5540f681a04d07fa2f256f",
            "943122676e564c4aad5126f4b1112dd6",
            "a6b52b390b0b4c3faa2a5892ff5ec77a"
          ]
        },
        "id": "iaVmk44xMeEY",
        "outputId": "8986ca77-c0fb-4f5c-dfde-ea73044af3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚è≥ Loading Mistral-7B model...\n",
            "   ‚è±Ô∏è  Expected load time: 1-2 minutes\n",
            "   ‚è±Ô∏è  Expected generation: 2-5 seconds per response\n",
            "   Device: cuda (NVIDIA A100-SXM4-80GB)\n",
            "   Torch: cuda (torch.bfloat16)\n",
            "   üì¶ Model size: ~14GB (will download on first run)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "213bd56f7fec433bb552a57c0d19c9c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Model loaded! Generating response...\n",
            "   ‚è±Ô∏è  This should take 2-5 seconds on GPU...\n",
            "\n",
            "‚úÖ Response generated in 2.17 seconds\n",
            "\n",
            "============================================================\n",
            "[{'generated_text': [{'role': 'system', 'content': 'You are Tom and I am Jerry'}, {'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a friendly and helpful AI assistant, here to help answer your questions and assist you in any way I can. While I'm not a cartoon character like Tom or Jerry, I'm happy to chat with you and help you out! How can I assist you today?\"}]}]\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Specify which Mistral model to use from Hugging Face\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# ‚ö†Ô∏è PERFORMANCE INFO:\n",
        "# Mistral-7B is a LARGE model (7 billion parameters, ~14GB)\n",
        "# Settings are automatically optimized based on device (CPU/GPU) detected above\n",
        "# The code automatically switches between CPU and GPU optimizations\n",
        "\n",
        "print(f\"\\n‚è≥ Loading Mistral-7B model...\")\n",
        "if device == \"cpu\":\n",
        "    print(f\"   ‚è±Ô∏è  Expected load time: 5-15 minutes\")\n",
        "    print(f\"   ‚è±Ô∏è  Expected generation: 30-60 seconds per response\")\n",
        "\n",
        "    device_info = \"Intel/AMD CPU\"\n",
        "    torch_dtype = torch.float32     # safest for CPUs\n",
        "\n",
        "    max_new_tokens = 256            # reduce memory usage on CPU\n",
        "\n",
        "else:  # GPU\n",
        "    print(f\"   ‚è±Ô∏è  Expected load time: 1-2 minutes\")\n",
        "    print(f\"   ‚è±Ô∏è  Expected generation: 2-5 seconds per response\")\n",
        "\n",
        "    device_info = torch.cuda.get_device_name(0)\n",
        "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    max_new_tokens = 512\n",
        "\n",
        "print(f\"   Device: {device} ({device_info})\")\n",
        "print(f\"   Torch: {device} ({torch_dtype})\")\n",
        "print(f\"   üì¶ Model size: ~14GB (will download on first run)\")\n",
        "\n",
        "# Create a conversation with system prompt and user message\n",
        "# System prompt defines the AI's role/personality\n",
        "# User message is what the person is asking\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Tom and I am Jerry\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "\n",
        "# Set up the text generation pipeline with device-optimized parameters\n",
        "# Settings automatically adapt based on device (CPU/GPU) detected in Cell 4\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",                              # Task type: generating text\n",
        "    model=model_id,                                 # Which model to use\n",
        "    token=hf_token,                                 # Authentication token for Hugging Face\n",
        "    dtype=torch_dtype,                              # Automatically set: bfloat16 (GPU) or float32 (CPU)\n",
        "    device_map=\"auto\",                              # Automatically use GPU if available\n",
        "    max_new_tokens=max_new_tokens,                  # Automatically set: 512 (GPU) or 256 (CPU)\n",
        "    do_sample=True,                                 # Use random sampling for more creative responses\n",
        "    top_k=10,                                       # Consider top 10 most likely next words\n",
        "    num_return_sequences=1,                         # Generate only 1 response\n",
        "    eos_token_id=2,                                 # Token ID that signals end of response\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded! Generating response...\")\n",
        "if device == \"cpu\":\n",
        "    print(\"   ‚è±Ô∏è  This may take 30-60 seconds on CPU...\")\n",
        "else:\n",
        "    print(\"   ‚è±Ô∏è  This should take 2-5 seconds on GPU...\")\n",
        "\n",
        "# Generate response using the pipeline and print the result\n",
        "import time\n",
        "start_time = time.time()\n",
        "result = chatbot(messages)\n",
        "generation_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Response generated in {generation_time:.2f} seconds\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(result)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rH6ue1EsX09S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da635fe-173a-440c-a274-5322e3f80512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In this conversation, I will be acting as Tom, while you will be Jerry. So, you are Jerry. Let the cat and mouse games begin!\n"
          ]
        }
      ],
      "source": [
        "# Generate the response and store the full result\n",
        "result = chatbot(messages)\n",
        "\n",
        "# Extract just the assistant's response from the complex output structure\n",
        "# result[0] gets the first (and only) generated sequence\n",
        "# [\"generated_text\"] gets the conversation history with the new response\n",
        "# [-1] gets the last message in the conversation (the assistant's reply)\n",
        "# [\"content\"] gets just the text content without the role information\n",
        "assistant_reply = result[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "# Print only the clean assistant response (without all the extra structure)\n",
        "print(assistant_reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f9a8d7f599c14147aa4e80177b20d0c4",
            "860c7175d48f4a1e8f6ae40256ec0a25",
            "daa6d738069741fabdc320bf16f69ae1",
            "3cd5bd1942dc4ca6b1b3f5b668b9eba8",
            "88d90c3bedc94cbb8ee79cd0e8a45bea",
            "1bb3f0f866cf49409f82d8ca9c5d89a6",
            "efdb17595ecf420183a80999d6218120",
            "29c12997d2de499dac89db30781ed4d6",
            "4d3e9c17720a43ad9efe705fde4ea7b0",
            "a19db27510764ca7ad1985ad1be61247",
            "7dbf62007ada4f4a81d72c80889285c5"
          ]
        },
        "id": "2bYm3yfvMcQc",
        "outputId": "14e47867-5f0a-4a34-b91a-83839f70c530"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9a8d7f599c14147aa4e80177b20d0c4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the tokenizer (converts text to numbers that the model understands)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "# Load the actual model with device-optimized settings\n",
        "# torch_dtype is automatically set in Cell 4: bfloat16 (GPU) or float32 (CPU)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,                    # Which model to load\n",
        "    token=hf_token,             # Authentication token\n",
        "    dtype=torch.bfloat16,       # Use 16-bit precision for faster processing\n",
        "    device_map=\"auto\"           # Automatically use GPU if available\n",
        ")\n",
        "\n",
        "# Create a simple conversation (just user input, no system prompt this time)\n",
        "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
        "\n",
        "# Convert the conversation into the format the model expects\n",
        "# This applies the model's chat template and converts to tensors\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    conversation,                # The conversation to format\n",
        "    add_generation_prompt=True,  # Add prompt to signal the model should respond\n",
        "    return_dict=True,           # Return as dictionary\n",
        "    return_tensors=\"pt\",        # Return as PyTorch tensors\n",
        ").to(model.device)             # Move to same device as model (GPU/CPU)\n",
        "\n",
        "# Generate the response using the model directly\n",
        "outputs = model.generate(\n",
        "    **inputs,                           # Pass all the formatted inputs\n",
        "    max_new_tokens=1000,               # Maximum length of response\n",
        "    pad_token_id=tokenizer.eos_token_id # Token to use for padding\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cMrFI6mVSJ6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1e6698-7f80-473a-f507-7626dcaaf9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1,     3,  2592, 29510, 29481,  1040,  8854,  1505,  1065,  6233,\n",
            "         29572,     4,  1083,  1717, 29510, 29475,  1274,  2121, 29501,  2304,\n",
            "         17353, 29493,  1347,  1083,  1309, 29510, 29475,  3852,  1040,  2636,\n",
            "          8854,  1065,  6233, 29491,  3761, 29493,  1083,  1309,  2680,  1136,\n",
            "          1137,  6233, 29493,  1505,  1956,  1070, 13495,  5611, 29493,  1427,\n",
            "          1032,  5794,  1148, 14761,  1062, 12027, 29491,  1183,  8854,  1117,\n",
            "         17351,  1163,  5160, 28408,  5942,  6241,  1040,  1647, 29491,  1183,\n",
            "          6868,  1142,  4138,  1228,  4980, 29493,  5166, 29493,  1072,  4396,\n",
            "         29493,  1163, 18759, 14131,  4822,  2169, 29473, 29518, 29502, 29501,\n",
            "         29518, 29550, 29670, 29511,  1093, 29552, 29551, 29501, 29555, 29555,\n",
            "         29670, 29533,  1377,  1183,  6024,  1142,  4138,  1228,  5693, 29493,\n",
            "          5392, 29493,  1072,  6121, 29493,  1163, 18759, 14131,  4822,  2169,\n",
            "         29473, 29538, 29501, 29551, 29670, 29511,  1093, 29538, 29555, 29501,\n",
            "         29549, 29552, 29670, 29533,  1377,  1429, 29510, 29481,  2511,  1032,\n",
            "          1947,  3796,  1066,  2645,  1032, 13995,  8854, 20680,  1927,  8162,\n",
            "          1032,  7364, 29491,     2]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Print the raw model output tensor (this shows token IDs/numbers, not readable text yet)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NhdQol6sUCOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8628b3fa-7b69-4989-dc81-8cca40b4dbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What's the weather like in Paris? I don't have real-time capabilities, so I can't provide the current weather in Paris. However, I can tell you that Paris, like much of northern France, has a temperate oceanic climate. The weather is mild with regular precipitation throughout the year. The warmest months are June, July, and August, with temperatures averaging around 20-25¬∞C (68-77¬∞F). The coldest months are December, January, and February, with temperatures averaging around 3-8¬∞C (37-46¬∞F). It's always a good idea to check a reliable weather forecast before planning a trip.\n"
          ]
        }
      ],
      "source": [
        "# Convert the token IDs back to readable text and print the result\n",
        "# outputs[0] gets the first generated sequence, skip_special_tokens removes formatting tokens\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Ypjagep9Zo"
      },
      "source": [
        "Class Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6NYvIaSp60y"
      },
      "source": [
        "## Step 1: Create an Agentic/Assistant System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p0TJu1LpqlQ"
      },
      "source": [
        "Choose a specific business context and create a system prompt that gives Mistral a professional role. This system prompt will define how the AI behaves and what expertise it has.\n",
        "\n",
        "**Instructions:**\n",
        "- Pick a realistic business or organization\n",
        "- Choose a specific role/expertise for the AI (marketing expert, technical consultant, etc.)\n",
        "- Create a system prompt that defines the AI's personality and knowledge area\n",
        "- This will be used throughout the assignment for generating content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iE9q7FtZnHC"
      },
      "outputs": [],
      "source": [
        "# TODO: Choose your business and role\n",
        "# Examples:\n",
        "# - \"TechStart Solutions - AI Consulting Firm\" with role \"AI Solutions Consultant\"\n",
        "# - \"Green Energy Corp - Solar Installation Company\" with role \"Solar Energy Expert\"\n",
        "# - \"HealthTech Plus - Medical Software Company\" with role \"Healthcare IT Specialist\"\n",
        "\n",
        "\n",
        "# Begin writing Python codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrpYw8WlqSQy"
      },
      "source": [
        "## Step 2: Generate Business Database Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSMlncXNqXNk"
      },
      "source": [
        "\n",
        "Use Mistral to create a comprehensive Q&A database for your chosen business. You'll prompt Mistral to generate realistic question-answer pairs that customers might ask about your services, pricing, processes, and expertise.\n",
        "\n",
        "**Instructions:**\n",
        "- Use your system prompt from Step 1 to give Mistral the business context\n",
        "- Create a prompt asking Mistral to generate 10-15 Q&A pairs for your business\n",
        "- Ask for questions covering different topics: services, pricing, processes, technical details, contact info\n",
        "- Format should be clear (Q: question, A: answer)\n",
        "- Parse the generated text into a usable list of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OToY7TUPqaOg"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate Q&A database using Mistral\n",
        "# You need to:\n",
        "# 1. Set up the Mistral model (use the pipeline approach from the original notebook)\n",
        "# 2. Create a function to get clean responses from Mistral\n",
        "# 3. Write a prompt asking Mistral to generate business Q&A pairs\n",
        "# 4. Parse the generated text into a list of dictionaries with 'question' and 'answer' keys\n",
        "# 5. Display your generated Q&A pairs clearly\n",
        "\n",
        "\n",
        "# Begin writing Python codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42vGxlCMrTlY"
      },
      "source": [
        "## Step 3: Implement FAISS Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8pg7QMYrOJ1"
      },
      "source": [
        "Convert your Q&A database into embeddings (numerical vectors) and store them in a FAISS index for fast similarity search. This allows users to ask questions and quickly find the most relevant information from your knowledge base.\n",
        "\n",
        "**Instructions:**\n",
        "- Install and import sentence-transformers for creating embeddings\n",
        "- Convert all your questions into numerical vectors using an embedding model\n",
        "- Create a FAISS index to store these vectors for fast similarity search\n",
        "- Implement a search function that can find similar questions based on user input\n",
        "- Test your search functionality with a sample query\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90whzGEJrNpQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement FAISS Vector Database\n",
        "# You need to:\n",
        "# 1. Install sentence-transformers: !pip install sentence-transformers faiss-cpu\n",
        "# 2. Import SentenceTransformer and faiss\n",
        "# 3. Load an embedding model (e.g., 'distilbert-base-uncased-distilled-squad')\n",
        "# 4. Extract questions and answers from your Q&A database\n",
        "# 5. Convert questions to embeddings using the model\n",
        "# 6. Create a FAISS index and add the embeddings\n",
        "# 7. Create a search function that takes a user question and returns similar Q&A pairs\n",
        "# 8. Test the search function with a sample query\n",
        "\n",
        "# Begin writing Python codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezDFPlX_sOok"
      },
      "source": [
        "## Step 4: Create Test Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwfORl3msT8w"
      },
      "source": [
        "Generate two types of questions to test your RAG system: questions that CAN be answered from your database (answerable) and questions that CANNOT be answered (unanswerable). This tests how well your system knows its limitations.\n",
        "\n",
        "**Instructions:**\n",
        "- Use Mistral to generate 5 questions that your business CAN answer (about your services, pricing, processes, etc.)\n",
        "- Use Mistral to generate 5 questions that your business CANNOT answer (competitor info, unrelated topics, personal details, etc.)\n",
        "- Extract the questions from the generated text into clean lists\n",
        "- These will test whether your RAG system correctly identifies when it can and cannot provide good answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p84cCeZsS7z"
      },
      "outputs": [],
      "source": [
        "# TODO: Create Test Questions\n",
        "# You need to:\n",
        "# 1. Generate ANSWERABLE questions using Mistral (questions your business can answer)\n",
        "# 2. Generate UNANSWERABLE questions using Mistral (questions outside your expertise)\n",
        "# 3. Parse both sets of questions into clean lists\n",
        "# 4. Display both types of questions clearly\n",
        "# 5. Make sure you have at least 5 questions of each type\n",
        "\n",
        "# Begin writing Python codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1PScUmtsi4u"
      },
      "source": [
        "## Step 5: Implement and Test Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Mx1i2_smB9"
      },
      "source": [
        "Run both types of questions through your RAG system and analyze how well it distinguishes between questions it can answer well versus questions it cannot answer reliably.\n",
        "\n",
        "**Instructions:**\n",
        "- Test your answerable questions - they should get high similarity scores with your database\n",
        "- Test your unanswerable questions - they should get low similarity scores\n",
        "- Set a similarity threshold to determine \"can answer\" vs \"cannot answer\"\n",
        "- Analyze the performance: did answerable questions score high? Did unanswerable questions score low?\n",
        "- Calculate accuracy rates for both question types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mOhEmMysqcd"
      },
      "outputs": [],
      "source": [
        "# TODO: Test Your RAG System\n",
        "# You need to:\n",
        "# 1. Create a testing function that searches your database for each question\n",
        "# 2. Set a similarity threshold (e.g., 0.7) to determine good vs poor matches\n",
        "# 3. Test all answerable questions and count how many are correctly identified as answerable\n",
        "# 4. Test all unanswerable questions and count how many are correctly identified as unanswerable\n",
        "# 5. Calculate and display performance statistics\n",
        "# 6. Show examples of good and poor matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dey4VWvusznu"
      },
      "source": [
        "## Step 6: Model Experimentation and Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIITyZQws6aP"
      },
      "source": [
        "Test multiple Q&A models from Hugging Face and rank them based on performance, speed, and confidence scores.\n",
        "\n",
        "**Instructions:**\n",
        "- Test the 4 required models plus 2 additional models of your choice\n",
        "- Evaluate each model on speed, confidence scores, and answer quality\n",
        "- Rank models from best to worst with clear explanations\n",
        "- Identify which models provide good confidence scores while maintaining reasonable output\n",
        "- Compare performance across different question types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWzWXLE2suzB"
      },
      "outputs": [],
      "source": [
        "# TODO: Test and Rank QA Models\n",
        "# Required models to test:\n",
        "# - \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\"\n",
        "# - \"deepset/roberta-base-squad2\"\n",
        "# - \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\"\n",
        "# - \"gasolsun/DynamicRAG-8B\"\n",
        "# Plus 2 additional QA models of your choice\n",
        "#\n",
        "# You need to:\n",
        "# 1. Set up QA pipelines for each model\n",
        "# 2. Test them with your questions and retrieved contexts\n",
        "# 3. Measure response time and confidence scores\n",
        "# 4. Rank models based on composite performance\n",
        "# 5. Identify models with good confidence handling\n",
        "# 6. Explain why each model ranked where it did\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Write your explanation here:\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm7hiX6VtWrI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "213bd56f7fec433bb552a57c0d19c9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48eb2041e1e64e2299d9fa262b9e2b44",
              "IPY_MODEL_a34839af6d964881ab34ddd6e8e314a0",
              "IPY_MODEL_7d72bf576c554ee39cdc38f619c6711b"
            ],
            "layout": "IPY_MODEL_c45108f07d4b4105a0a31a0ac0da00d6"
          }
        },
        "48eb2041e1e64e2299d9fa262b9e2b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c592cbd84d34e5d85645ed55013ac9b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aec94d73cc3747edabc03453dc0ba15f",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "a34839af6d964881ab34ddd6e8e314a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1056aa7715fb4f5bba250e5513d8fad8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5766e803b5540f681a04d07fa2f256f",
            "value": 3
          }
        },
        "7d72bf576c554ee39cdc38f619c6711b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943122676e564c4aad5126f4b1112dd6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a6b52b390b0b4c3faa2a5892ff5ec77a",
            "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.39s/it]"
          }
        },
        "c45108f07d4b4105a0a31a0ac0da00d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c592cbd84d34e5d85645ed55013ac9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec94d73cc3747edabc03453dc0ba15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1056aa7715fb4f5bba250e5513d8fad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5766e803b5540f681a04d07fa2f256f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "943122676e564c4aad5126f4b1112dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b52b390b0b4c3faa2a5892ff5ec77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9a8d7f599c14147aa4e80177b20d0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_860c7175d48f4a1e8f6ae40256ec0a25",
              "IPY_MODEL_daa6d738069741fabdc320bf16f69ae1",
              "IPY_MODEL_3cd5bd1942dc4ca6b1b3f5b668b9eba8"
            ],
            "layout": "IPY_MODEL_88d90c3bedc94cbb8ee79cd0e8a45bea"
          }
        },
        "860c7175d48f4a1e8f6ae40256ec0a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bb3f0f866cf49409f82d8ca9c5d89a6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_efdb17595ecf420183a80999d6218120",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "daa6d738069741fabdc320bf16f69ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c12997d2de499dac89db30781ed4d6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d3e9c17720a43ad9efe705fde4ea7b0",
            "value": 3
          }
        },
        "3cd5bd1942dc4ca6b1b3f5b668b9eba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a19db27510764ca7ad1985ad1be61247",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7dbf62007ada4f4a81d72c80889285c5",
            "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.38s/it]"
          }
        },
        "88d90c3bedc94cbb8ee79cd0e8a45bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb3f0f866cf49409f82d8ca9c5d89a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efdb17595ecf420183a80999d6218120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29c12997d2de499dac89db30781ed4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3e9c17720a43ad9efe705fde4ea7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a19db27510764ca7ad1985ad1be61247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbf62007ada4f4a81d72c80889285c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}