{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "969S__pMmA4o"
   },
   "source": [
    "# Prompt Engineering Activity\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "- **HuggingFace Token**: Get from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- **Google Colab Account** (Free tier works!) - *Optional, can run locally too*\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Quick Links\n",
    "\n",
    "**üîó Open in Colab**: [Click here](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)\n",
    "\n",
    "**üìÇ View on GitHub**: [Click here](https://github.com/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)\n",
    "\n",
    "> **‚ö†Ô∏è Note**: Colab link requires the repository to be public on GitHub. If you get a 404 error, see troubleshooting below.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Setup Instructions\n",
    "\n",
    "### Option 1: Google Colab (Recommended for GPU)\n",
    "\n",
    "#### Step 1: Open Notebook\n",
    "- **Method A**: Click the \"Open in Colab\" link above\n",
    "- **Method B**: \n",
    "  1. Go to [Google Colab](https://colab.research.google.com/)\n",
    "  2. Click **File** ‚Üí **Open notebook** ‚Üí **GitHub** tab\n",
    "  3. Enter: `oviya-raja/ist-402-assignments`\n",
    "  4. Navigate to: `IST402/assignments/W3/reference/W3__Prompt_Engineering w_QA Applications-2.ipynb`\n",
    "\n",
    "#### Step 2: Enable GPU (Recommended)\n",
    "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
    "2. Select **GPU** ‚Üí **Save**\n",
    "3. **Runtime** ‚Üí **Restart runtime**\n",
    "\n",
    "#### Step 3: Set Up Token (See Token Setup section below)\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Local Environment\n",
    "\n",
    "1. **Install dependencies**: Run Cell 2 (Install packages)\n",
    "2. **Set up token**: See Token Setup section below\n",
    "3. **Run cells in order**: Start from Cell 1\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Token Setup\n",
    "\n",
    "### For Google Colab Users\n",
    "\n",
    "**Recommended Method: Using .env file**\n",
    "1. Run **Cell 4** ‚Üí It will automatically create a `.env` file\n",
    "2. Click the **folder icon (üìÅ)** in the left sidebar\n",
    "3. Find and click `.env` file\n",
    "4. Replace `your_token_here` with your actual token\n",
    "5. Save (Ctrl+S or Cmd+S)\n",
    "6. Re-run **Cell 4** ‚Üí Token loaded! ‚úÖ\n",
    "\n",
    "**Quick Method: Direct environment variable**\n",
    "Run this in a new cell before Cell 4:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_actual_token_here\"\n",
    "```\n",
    "\n",
    "### For Local Users\n",
    "\n",
    "**Create `.env` file manually:**\n",
    "1. Create a file named `.env` in the same directory as this notebook\n",
    "2. Add this line: `HUGGINGFACE_HUB_TOKEN=your_actual_token_here`\n",
    "3. No spaces around the `=` sign!\n",
    "4. Run **Cell 4** ‚Üí Token loaded! ‚úÖ\n",
    "\n",
    "**Get your token**: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "> **üìñ Learn more**: See `ENV_IN_COLAB.md` for detailed explanation of how `.env` files work in Colab\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Troubleshooting\n",
    "\n",
    "### 404 Error When Opening from GitHub\n",
    "\n",
    "**Possible causes:**\n",
    "- Repository doesn't exist yet ‚Üí Use **Option 2** (Upload to Colab)\n",
    "- Repository is private ‚Üí Make it public or use **Option 2**\n",
    "- Wrong branch ‚Üí Try changing `main` to `master` in the link\n",
    "\n",
    "**Solution**: Upload the notebook directly to Colab:\n",
    "1. Download this notebook\n",
    "2. Go to [Google Colab](https://colab.research.google.com/)\n",
    "3. Click **File** ‚Üí **Upload notebook**\n",
    "4. Select the downloaded file\n",
    "\n",
    "### GPU Not Detected in Colab\n",
    "\n",
    "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
    "2. Select **GPU** ‚Üí **Save**\n",
    "3. **Runtime** ‚Üí **Restart runtime**\n",
    "4. Re-run Cell 1 to verify\n",
    "\n",
    "### Token Not Loading\n",
    "\n",
    "- Check that `.env` file exists and has correct format: `HUGGINGFACE_HUB_TOKEN=token` (no spaces)\n",
    "- Make sure you re-ran Cell 4 after creating/editing `.env`\n",
    "- Verify token is valid at [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Getting Started\n",
    "\n",
    "1. **Run Cell 1**: Verify environment setup\n",
    "2. **Run Cell 2**: Install required packages\n",
    "3. **Run Cell 3**: Set up FAISS (CPU/GPU)\n",
    "4. **Run Cell 4**: Set up Hugging Face token\n",
    "5. **Continue**: Run remaining cells in order\n",
    "\n",
    "**Happy coding! üéâ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Google Colab environment...\n",
      "   Python version: 3.11.14\n",
      "   ‚ö†Ô∏è  Not running in Google Colab (local environment)\n",
      "   ‚ö†Ô∏è  GPU NOT detected\n",
      "\n",
      "üìã Next Steps:\n",
      "   1. If GPU not detected in Colab: Enable GPU runtime and restart\n",
      "   2. Run Cell 2: Install packages\n",
      "   3. Run Cell 3: Set up Hugging Face token\n",
      "   4. Continue with remaining cells\n"
     ]
    }
   ],
   "source": [
    "# Google Colab Setup Verification\n",
    "# Run this cell FIRST to check if everything is set up correctly\n",
    "\n",
    "import sys\n",
    "print(\"üîç Checking Google Colab environment...\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"   ‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"   ‚ö†Ô∏è  Not running in Google Colab (local environment)\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  GPU NOT detected\")\n",
    "        if IN_COLAB:\n",
    "            print(\"   üí° TIP: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
    "            print(\"   üí° Then: Runtime ‚Üí Restart runtime\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  PyTorch not installed yet (will be installed in next cell)\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. If GPU not detected in Colab: Enable GPU runtime and restart\")\n",
    "print(\"   2. Run Cell 2: Install packages\")\n",
    "print(\"   3. Run Cell 3: Set up Hugging Face token\")\n",
    "print(\"   4. Continue with remaining cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q16cFgD3MfHJ",
    "outputId": "88707cc5-b0c8-4afc-8a63-ba1a58bf4002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vscode/.local/lib/python3.11/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/vscode/.local/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/vscode/.local/lib/python3.11/site-packages (5.1.2)\n",
      "Requirement already satisfied: datasets in /home/vscode/.local/lib/python3.11/site-packages (4.4.1)\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scikit-learn in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: Pillow in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/vscode/.local/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vscode/.local/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/vscode/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vscode/.local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages - run this cell first\n",
    "# Note: FAISS package will be installed conditionally based on GPU availability in Cell 3\n",
    "\n",
    "# Core packages (always needed)\n",
    "%pip install transformers torch sentence-transformers datasets python-dotenv\n",
    "\n",
    "# FAISS will be installed conditionally in Cell 3 based on device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G-MQ5uhON96Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detected: Local environment\n",
      "‚úÖ Hugging Face token loaded successfully!\n",
      "   Token preview: hf_ThdSIol...ustv\n",
      "   Source: .env file\n"
     ]
    }
   ],
   "source": [
    "# Set up Hugging Face Token - Loads from .env file (secure method)\n",
    "# This cell automatically handles both Colab and local environments\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîç Detected: Google Colab environment\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üîç Detected: Local environment\")\n",
    "\n",
    "# Method 1: Try to load from .env file (works for both Colab and local)\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Method 2: If .env doesn't exist and we're in Colab, provide instructions\n",
    "if not hf_token and IN_COLAB:\n",
    "    print(\"\\nüìù .env file not found in Google Colab\")\n",
    "    print(\"   Creating .env file template...\")\n",
    "    \n",
    "    # Create .env file with placeholder\n",
    "    try:\n",
    "        env_content = \"HUGGINGFACE_HUB_TOKEN=your_token_here\\n\"\n",
    "        with open('.env', 'w') as f:\n",
    "            f.write(env_content)\n",
    "        \n",
    "        print(\"‚úÖ .env file created!\")\n",
    "        print(\"‚ö†Ô∏è  IMPORTANT: Edit the .env file in the left sidebar\")\n",
    "        print(\"   1. Click on .env file in the file browser\")\n",
    "        print(\"   2. Replace 'your_token_here' with your actual token\")\n",
    "        print(\"   3. Get token from: https://huggingface.co/settings/tokens\")\n",
    "        print(\"   4. Re-run this cell after editing\")\n",
    "        print(\"\\n   OR use Option B (direct method) below:\")\n",
    "        print(\"   \" + \"=\"*60)\n",
    "        print(\"   # Quick setup - run this in a new cell:\")\n",
    "        print(\"   import os\")\n",
    "        print(\"   os.environ['HUGGINGFACE_HUB_TOKEN'] = 'your_actual_token_here'\")\n",
    "        print(\"   # Then re-run this cell\")\n",
    "        print(\"   \" + \"=\"*60)\n",
    "        \n",
    "        # Reload after creation\n",
    "        load_dotenv()\n",
    "        hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not create .env file: {e}\")\n",
    "\n",
    "# Method 3: Check if token is already set as environment variable (for Colab quick setup)\n",
    "if not hf_token:\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Final check\n",
    "if not hf_token or hf_token == \"your_token_here\" or hf_token == \"your_actual_token_here\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö†Ô∏è  HUGGINGFACE_HUB_TOKEN not found or not set!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüìã Setup Instructions:\")\n",
    "    print(\"\\n   For Local Environment:\")\n",
    "    print(\"   1. Create a .env file in the same directory as this notebook\")\n",
    "    print(\"   2. Add this line: HUGGINGFACE_HUB_TOKEN=your_actual_token_here\")\n",
    "    print(\"   3. No spaces around the = sign!\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "    print(\"\\n   For Google Colab:\")\n",
    "    print(\"   1. Get token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"   2. Run this in a new cell:\")\n",
    "    print(\"      import os\")\n",
    "    print(\"      os.environ['HUGGINGFACE_HUB_TOKEN'] = 'your_actual_token_here'\")\n",
    "    print(\"   3. Re-run this cell\")\n",
    "    print(\"\\n   Get your token: https://huggingface.co/settings/tokens\")\n",
    "    print(\"=\"*60)\n",
    "    raise ValueError(\"HUGGINGFACE_HUB_TOKEN not configured. Please follow instructions above.\")\n",
    "else:\n",
    "    print(\"‚úÖ Hugging Face token loaded successfully!\")\n",
    "    print(f\"   Token preview: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else '****'}\")\n",
    "    print(f\"   Source: {'Environment variable' if os.environ.get('HUGGINGFACE_HUB_TOKEN') and not os.path.exists('.env') else '.env file'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXMPHypoVUUS",
    "outputId": "ebbe85dd-03d4-4583-a219-df3195ea4e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries we need\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYOyg2_clJAL",
    "outputId": "155d67cd-d1ae-45d0-afd5-8f33538d11b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect and configure device (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "2e66c9c6dbbe42dab15c1a5e82d60a8e",
      "c6f37277df9b463fb54164fe9b7514a8",
      "b2ef6f6df0714453b487d9917006d3e8",
      "1d772808c87040589a88880dfa05cf1a",
      "b89a692247044f059e198a652e647e84",
      "f1840e10fb5f4aee95c9f10dae7f3beb",
      "35854eead495446d80272f9803a2989b",
      "a3e19142299342a98d3bdb1e597e51d2",
      "263913cf89b54b689e6942f06407cc6b",
      "4d0f1045fb5f49be9eed0c67d4390507",
      "6fd6de6400a24486a55f5ad3c4ab7dbc"
     ]
    },
    "id": "iaVmk44xMeEY",
    "outputId": "260f70dd-025d-4934-d262-abee8bee970a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Loading Mistral-7B model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ‚ö†Ô∏è PERFORMANCE INFO:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Mistral-7B is a LARGE model (7 billion parameters, ~14GB)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Settings are automatically optimized based on device (CPU/GPU) detected above\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# The code automatically switches between CPU and GPU optimizations\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚è≥ Loading Mistral-7B model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdevice_info\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚è±Ô∏è  Expected load time: 5-15 minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'device_info' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify which Mistral model to use from Hugging Face\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# ‚ö†Ô∏è PERFORMANCE INFO:\n",
    "# Mistral-7B is a LARGE model (7 billion parameters, ~14GB)\n",
    "# Settings are automatically optimized based on device (CPU/GPU) detected above\n",
    "# The code automatically switches between CPU and GPU optimizations\n",
    "\n",
    "print(f\"\\n‚è≥ Loading Mistral-7B model...\")\n",
    "print(f\"   Device: {device} ({device_info})\")\n",
    "if device == \"cpu\":\n",
    "    print(f\"   ‚è±Ô∏è  Expected load time: 5-15 minutes\")\n",
    "    print(f\"   ‚è±Ô∏è  Expected generation: 30-60 seconds per response\")\n",
    "else:\n",
    "    print(f\"   ‚è±Ô∏è  Expected load time: 1-2 minutes\")\n",
    "    print(f\"   ‚è±Ô∏è  Expected generation: 2-5 seconds per response\")\n",
    "print(f\"   üì¶ Model size: ~14GB (will download on first run)\")\n",
    "\n",
    "# Create a conversation with system prompt and user message\n",
    "# System prompt defines the AI's role/personality\n",
    "# User message is what the person is asking\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Set up the text generation pipeline with device-optimized parameters\n",
    "# Settings automatically adapt based on device (CPU/GPU) detected in Cell 4\n",
    "chatbot = pipeline(\n",
    "    \"text-generation\",                              # Task type: generating text\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",   # Which model to use\n",
    "    token=hf_token,                                 # Authentication token for Hugging Face\n",
    "    dtype=torch_dtype,                              # Automatically set: bfloat16 (GPU) or float32 (CPU)\n",
    "    device_map=\"auto\",                              # Automatically use GPU if available\n",
    "    max_new_tokens=max_new_tokens,                  # Automatically set: 512 (GPU) or 256 (CPU)\n",
    "    do_sample=True,                                 # Use random sampling for more creative responses\n",
    "    top_k=10,                                       # Consider top 10 most likely next words\n",
    "    num_return_sequences=1,                         # Generate only 1 response\n",
    "    eos_token_id=2,                                 # Token ID that signals end of response\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded! Generating response...\")\n",
    "if device == \"cpu\":\n",
    "    print(\"   ‚è±Ô∏è  This may take 30-60 seconds on CPU...\")\n",
    "else:\n",
    "    print(\"   ‚è±Ô∏è  This should take 2-5 seconds on GPU...\")\n",
    "\n",
    "# Generate response using the pipeline and print the result\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = chatbot(messages)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Response generated in {generation_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH6ue1EsX09S",
    "outputId": "695c8f48-70d1-44df-e13a-26ec21af891a"
   },
   "outputs": [],
   "source": [
    "# Generate the response and store the full result\n",
    "result = chatbot(messages)\n",
    "\n",
    "# Extract just the assistant's response from the complex output structure\n",
    "# result[0] gets the first (and only) generated sequence\n",
    "# [\"generated_text\"] gets the conversation history with the new response\n",
    "# [-1] gets the last message in the conversation (the assistant's reply)\n",
    "# [\"content\"] gets just the text content without the role information\n",
    "assistant_reply = result[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "# Print only the clean assistant response (without all the extra structure)\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cc7065e0a77942ccb964aa408b2c9df2",
      "dc216ef7a3814759b49a999ee7c21eec",
      "9a0f537660bd40e2a7dd0efab22a5659",
      "d2990d0490214324b6860fc14d598139",
      "c11c029bbe4e4463a16ef91d80476f85",
      "b3b2cd7cedca464f85a218985bc477b7",
      "0a814618a94040baa4e75674403fb487",
      "7aa0e4a1059943dc8574c479421055e4",
      "1ec60428a2d744b7ae8d5754665006ab",
      "e219a8985b3f4d55a64c4a342cf046d1",
      "c93c153489ca4e85b236d10a03cb68cc"
     ]
    },
    "id": "2bYm3yfvMcQc",
    "outputId": "ec8ff679-9355-452c-84d9-6f091ce00428"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae2a3ad38b34ae5a537de2aec15688a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# Your Hugging Face authentication token (replace with your actual token)\n",
    "hf_token = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
    "\n",
    "# Specify the Mistral model we want to use\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Load the tokenizer (converts text to numbers that the model understands)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "# Load the actual model with device-optimized settings\n",
    "# torch_dtype is automatically set in Cell 4: bfloat16 (GPU) or float32 (CPU)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,                    # Which model to load\n",
    "    token=hf_token,             # Authentication token\n",
    "    dtype=torch.bfloat16,       # Use 16-bit precision for faster processing\n",
    "    device_map=\"auto\"           # Automatically use GPU if available\n",
    ")\n",
    "\n",
    "# Create a simple conversation (just user input, no system prompt this time)\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "\n",
    "# Convert the conversation into the format the model expects\n",
    "# This applies the model's chat template and converts to tensors\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    conversation,                # The conversation to format\n",
    "    add_generation_prompt=True,  # Add prompt to signal the model should respond\n",
    "    return_dict=True,           # Return as dictionary\n",
    "    return_tensors=\"pt\",        # Return as PyTorch tensors\n",
    ").to(model.device)             # Move to same device as model (GPU/CPU)\n",
    "\n",
    "# Generate the response using the model directly\n",
    "outputs = model.generate(\n",
    "    **inputs,                           # Pass all the formatted inputs\n",
    "    max_new_tokens=1000,               # Maximum length of response\n",
    "    pad_token_id=tokenizer.eos_token_id # Token to use for padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMrFI6mVSJ6R",
    "outputId": "873131c6-ac87-4e39-e4da-48f159518fbb"
   },
   "outputs": [],
   "source": [
    "# Print the raw model output tensor (this shows token IDs/numbers, not readable text yet)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhdQol6sUCOH",
    "outputId": "ff3a0499-32b5-4f8f-b7ec-fab291a3df98"
   },
   "outputs": [],
   "source": [
    "# Convert the token IDs back to readable text and print the result\n",
    "# outputs[0] gets the first generated sequence, skip_special_tokens removes formatting tokens\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7Ypjagep9Zo"
   },
   "source": [
    "Class Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6NYvIaSp60y"
   },
   "source": [
    "## Step 1: Create an Agentic/Assistant System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p0TJu1LpqlQ"
   },
   "source": [
    "Choose a specific business context and create a system prompt that gives Mistral a professional role. This system prompt will define how the AI behaves and what expertise it has.\n",
    "\n",
    "**Instructions:**\n",
    "- Pick a realistic business or organization\n",
    "- Choose a specific role/expertise for the AI (marketing expert, technical consultant, etc.)\n",
    "- Create a system prompt that defines the AI's personality and knowledge area\n",
    "- This will be used throughout the assignment for generating content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iE9q7FtZnHC"
   },
   "outputs": [],
   "source": [
    "# TODO: Choose your business and role\n",
    "# Examples:\n",
    "# - \"TechStart Solutions - AI Consulting Firm\" with role \"AI Solutions Consultant\"\n",
    "# - \"Green Energy Corp - Solar Installation Company\" with role \"Solar Energy Expert\"\n",
    "# - \"HealthTech Plus - Medical Software Company\" with role \"Healthcare IT Specialist\"\n",
    "\n",
    "\n",
    "# Begin writing Python codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrpYw8WlqSQy"
   },
   "source": [
    "## Step 2: Generate Business Database Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSMlncXNqXNk"
   },
   "source": [
    "\n",
    "Use Mistral to create a comprehensive Q&A database for your chosen business. You'll prompt Mistral to generate realistic question-answer pairs that customers might ask about your services, pricing, processes, and expertise.\n",
    "\n",
    "**Instructions:**\n",
    "- Use your system prompt from Step 1 to give Mistral the business context\n",
    "- Create a prompt asking Mistral to generate 10-15 Q&A pairs for your business\n",
    "- Ask for questions covering different topics: services, pricing, processes, technical details, contact info\n",
    "- Format should be clear (Q: question, A: answer)\n",
    "- Parse the generated text into a usable list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OToY7TUPqaOg"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate Q&A database using Mistral\n",
    "# You need to:\n",
    "# 1. Set up the Mistral model (use the pipeline approach from the original notebook)\n",
    "# 2. Create a function to get clean responses from Mistral\n",
    "# 3. Write a prompt asking Mistral to generate business Q&A pairs\n",
    "# 4. Parse the generated text into a list of dictionaries with 'question' and 'answer' keys\n",
    "# 5. Display your generated Q&A pairs clearly\n",
    "\n",
    "\n",
    "# Begin writing Python codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42vGxlCMrTlY"
   },
   "source": [
    "## Step 3: Implement FAISS Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8pg7QMYrOJ1"
   },
   "source": [
    "Convert your Q&A database into embeddings (numerical vectors) and store them in a FAISS index for fast similarity search. This allows users to ask questions and quickly find the most relevant information from your knowledge base.\n",
    "\n",
    "**Instructions:**\n",
    "- Install and import sentence-transformers for creating embeddings\n",
    "- Convert all your questions into numerical vectors using an embedding model\n",
    "- Create a FAISS index to store these vectors for fast similarity search\n",
    "- Implement a search function that can find similar questions based on user input\n",
    "- Test your search functionality with a sample query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90whzGEJrNpQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement FAISS Vector Database\n",
    "# You need to:\n",
    "# 1. Install sentence-transformers: !pip install sentence-transformers faiss-cpu\n",
    "# 2. Import SentenceTransformer and faiss\n",
    "# 3. Load an embedding model (e.g., 'distilbert-base-uncased-distilled-squad')\n",
    "# 4. Extract questions and answers from your Q&A database\n",
    "# 5. Convert questions to embeddings using the model\n",
    "# 6. Create a FAISS index and add the embeddings\n",
    "# 7. Create a search function that takes a user question and returns similar Q&A pairs\n",
    "# 8. Test the search function with a sample query\n",
    "\n",
    "# Begin writing Python codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezDFPlX_sOok"
   },
   "source": [
    "## Step 4: Create Test Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwfORl3msT8w"
   },
   "source": [
    "Generate two types of questions to test your RAG system: questions that CAN be answered from your database (answerable) and questions that CANNOT be answered (unanswerable). This tests how well your system knows its limitations.\n",
    "\n",
    "**Instructions:**\n",
    "- Use Mistral to generate 5 questions that your business CAN answer (about your services, pricing, processes, etc.)\n",
    "- Use Mistral to generate 5 questions that your business CANNOT answer (competitor info, unrelated topics, personal details, etc.)\n",
    "- Extract the questions from the generated text into clean lists\n",
    "- These will test whether your RAG system correctly identifies when it can and cannot provide good answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p84cCeZsS7z"
   },
   "outputs": [],
   "source": [
    "# TODO: Create Test Questions\n",
    "# You need to:\n",
    "# 1. Generate ANSWERABLE questions using Mistral (questions your business can answer)\n",
    "# 2. Generate UNANSWERABLE questions using Mistral (questions outside your expertise)\n",
    "# 3. Parse both sets of questions into clean lists\n",
    "# 4. Display both types of questions clearly\n",
    "# 5. Make sure you have at least 5 questions of each type\n",
    "\n",
    "# Begin writing Python codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1PScUmtsi4u"
   },
   "source": [
    "## Step 5: Implement and Test Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7Mx1i2_smB9"
   },
   "source": [
    "Run both types of questions through your RAG system and analyze how well it distinguishes between questions it can answer well versus questions it cannot answer reliably.\n",
    "\n",
    "**Instructions:**\n",
    "- Test your answerable questions - they should get high similarity scores with your database\n",
    "- Test your unanswerable questions - they should get low similarity scores\n",
    "- Set a similarity threshold to determine \"can answer\" vs \"cannot answer\"\n",
    "- Analyze the performance: did answerable questions score high? Did unanswerable questions score low?\n",
    "- Calculate accuracy rates for both question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mOhEmMysqcd"
   },
   "outputs": [],
   "source": [
    "# TODO: Test Your RAG System\n",
    "# You need to:\n",
    "# 1. Create a testing function that searches your database for each question\n",
    "# 2. Set a similarity threshold (e.g., 0.7) to determine good vs poor matches\n",
    "# 3. Test all answerable questions and count how many are correctly identified as answerable\n",
    "# 4. Test all unanswerable questions and count how many are correctly identified as unanswerable\n",
    "# 5. Calculate and display performance statistics\n",
    "# 6. Show examples of good and poor matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dey4VWvusznu"
   },
   "source": [
    "## Step 6: Model Experimentation and Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIITyZQws6aP"
   },
   "source": [
    "Test multiple Q&A models from Hugging Face and rank them based on performance, speed, and confidence scores.\n",
    "\n",
    "**Instructions:**\n",
    "- Test the 4 required models plus 2 additional models of your choice\n",
    "- Evaluate each model on speed, confidence scores, and answer quality\n",
    "- Rank models from best to worst with clear explanations\n",
    "- Identify which models provide good confidence scores while maintaining reasonable output\n",
    "- Compare performance across different question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vWzWXLE2suzB",
    "outputId": "6616d00f-0f6c-44f2-cfce-6bb7b668452b"
   },
   "outputs": [],
   "source": [
    "# TODO: Test and Rank QA Models\n",
    "# Required models to test:\n",
    "# - \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\"\n",
    "# - \"deepset/roberta-base-squad2\"\n",
    "# - \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\"\n",
    "# - \"gasolsun/DynamicRAG-8B\"\n",
    "# Plus 2 additional QA models of your choice\n",
    "#\n",
    "# You need to:\n",
    "# 1. Set up QA pipelines for each model\n",
    "# 2. Test them with your questions and retrieved contexts\n",
    "# 3. Measure response time and confidence scores\n",
    "# 4. Rank models based on composite performance\n",
    "# 5. Identify models with good confidence handling\n",
    "# 6. Explain why each model ranked where it did\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Write your explanation here:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cm7hiX6VtWrI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}