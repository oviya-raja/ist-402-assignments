{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agentic RAG with LlamaIndex - Complete Notebook Content\n",
        "\n",
        "This notebook contains all lessons from the course on building agentic RAG systems using LlamaIndex."
      ],
      "metadata": {
        "id": "9bqVkxISRAGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Installation\n",
        "First, let's install the required packages."
      ],
      "metadata": {
        "id": "RVenqsSbRJGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Installation\n",
        "First, let's install the required packages."
      ],
      "metadata": {
        "id": "E3m9sre_RQoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install nest-asyncio\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk2nfXE6RFva",
        "outputId": "a30ddc7a-983e-4ef6-b7dd-9a452247100c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (0.14.6)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.6 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.14.6)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.6.6)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.4)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (3.13.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.8.3)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama-index) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.109.1)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.10.5)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.1.3)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama-index) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama-index) (3.0.3)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.12/dist-packages (0.6.6)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-openai) (0.14.6)\n",
            "Requirement already satisfied: openai<2,>=1.108.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-openai) (1.109.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.13.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.8.3)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.17.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.108.1->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2,>=1.108.1->llama-index-llms-openai) (3.11)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.0.3)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai) (0.14.6)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai) (1.109.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.13.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.8.3)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.5.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.17.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.11)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.0.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up OpenAI API Key"
      ],
      "metadata": {
        "id": "aSJABWx7Rb77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up OpenAI API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# You can either set it directly or use getpass for security\n",
        "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w75jomMERZHb",
        "outputId": "214899eb-6d80-4644-c008-24ce3a942df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "JCcEpZn5RkLU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: Router Engine"
      ],
      "metadata": {
        "id": "SDa_MkmeRzEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "Download the MetaGPT paper:"
      ],
      "metadata": {
        "id": "MlO-_ImmR8u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWJozRpR3KM",
        "outputId": "3ec00f0e-512a-4ada-d9d5-000e5e3fb743"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 17:26:26--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  68.3MB/s    in 0.2s    \n",
            "\n",
            "2025-10-28 17:26:27 (68.3 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "urqMuc3uSF0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define LLM and Embedding Model"
      ],
      "metadata": {
        "id": "UKf_IJp-TMah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "T-2smc6ITQUh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "fk6roZbpTR_h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Summary Index and Vector Index"
      ],
      "metadata": {
        "id": "ZxpZND2rTUbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "KbdFzj_GTWPz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Query Engines and Set Metadata"
      ],
      "metadata": {
        "id": "HBLsNW0YTYUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "tBy7Qd_gTafp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "lFLi0Z32TdSa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Router Query Engine"
      ],
      "metadata": {
        "id": "di42S-cSTgFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "4c_U0uO_TfeK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ9zrRopTjiJ",
        "outputId": "9fda8d22-7294-4515-f437-dfc96e542574"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It outlines the roles of various agents in the software development process and discusses the efficient workflows, structured communication interfaces, and executable feedback mechanism incorporated in the framework. Through extensive experiments, MetaGPT demonstrates state-of-the-art performance on different benchmarks, showcasing its potential for developing LLM-based multi-agent systems. The document also provides insights into the development process of a software application called the \"Drawing App\" using MetaGPT, highlighting stages like requirements gathering, UI design, system architecture, implementation approach, testing, and task allocation. It discusses the use of Python libraries for GUI design, the role of different agents in the development process, the performance of MetaGPT in generating executable code, and the importance of structured messaging and feedback mechanisms. Additionally, it addresses limitations, ethical concerns, and potential future directions for the framework.\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qbzrYinTm38",
        "outputId": "88faf2a7-74cf-4fe4-e75f-97b51b0adf5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context from the MetaGPT paper, which would likely include information on how agents share information with other agents..\n",
            "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages and subscribe to relevant messages based on their profiles. This shared message pool allows all agents to exchange messages directly, enabling them to access messages from other entities transparently. By storing information in this global message pool, agents can retrieve required information without the need to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Tool Calling"
      ],
      "metadata": {
        "id": "I7P2fB-CTvr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define a Simple Tool"
      ],
      "metadata": {
        "id": "60323azIT4lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
      ],
      "metadata": {
        "id": "PZ8ZXkAQTxLD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "response = llm.predict_and_call(\n",
        "    [add_tool, mystery_tool],\n",
        "    \"Tell me the output of the mystery function on 2 and 9\",\n",
        "    verbose=True\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcGPDXyT7oM",
        "outputId": "824b01b8-0572-4041-bbd7-3d069bb0ea03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
            "=== Function Output ===\n",
            "121\n",
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define an Auto-Retrieval Tool"
      ],
      "metadata": {
        "id": "Bk-Waqk2T-FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "5O_iv2aTT_fz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjN83C8UCIz",
        "outputId": "5ec5f732-ac37-442b-caa0-3311e737779c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "file_name: metagpt.pdf\n",
            "file_path: metagpt.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 16911937\n",
            "creation_date: 2025-10-28\n",
            "last_modified_date: 2025-10-28\n",
            "\n",
            "Preprint\n",
            "METAGPT: M ETA PROGRAMMING FOR A\n",
            "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
            "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
            "Ceyao Zhang4, Jinlin Wang1, Zili Wang, Steven Ka Shing Yau5, Zijuan Lin4,\n",
            "Liyang Zhou6, Chenyu Ran1, Lingfeng Xiao1,7, Chenglin Wu1†, J¨urgen Schmidhuber2,8\n",
            "1DeepWisdom, 2AI Initiative, King Abdullah University of Science and Technology,\n",
            "3Xiamen University, 4The Chinese University of Hong Kong, Shenzhen,\n",
            "5Nanjing University, 6University of Pennsylvania,\n",
            "7University of California, Berkeley, 8The Swiss AI Lab IDSIA/USI/SUPSI\n",
            "ABSTRACT\n",
            "Remarkable progress has been made on automated problem solving through so-\n",
            "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
            "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
            "complex tasks, however, are complicated through logic inconsistencies due to\n",
            "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
            "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
            "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
            "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
            "streamlined workflows, thus allowing agents with human-like domain expertise\n",
            "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
            "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
            "complex tasks into subtasks involving many agents working together. On col-\n",
            "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
            "solutions than previous chat-based multi-agent systems. Our project can be found\n",
            "at https://github.com/geekan/MetaGPT.\n",
            "1 I NTRODUCTION\n",
            "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
            "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
            "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
            "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
            "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
            "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
            "et al., 2023; Qian et al., 2023).\n",
            "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
            "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
            "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
            "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
            "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
            "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
            "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
            "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
            "(PRDs) using a standardized structure, to guide the developmental process.\n",
            "Inspired by such ideas, we design a promising GPT -based Meta-Programming framework called\n",
            "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
            "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
            "∗These authors contributed equally to this work.\n",
            "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "2_O-RStXUDjM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    filters=MetadataFilters.from_dicts(\n",
        "        [\n",
        "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")\n",
        "print(str(response))\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyExE-bUFSk",
        "outputId": "44577393-554b-4d3a-d7f7-84ba651fa9bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some high-level results of MetaGPT include achieving a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1, outperforming other popular frameworks like AutoGPT, LangChain, AgentVerse, and ChatDev. Additionally, MetaGPT demonstrates robustness and efficiency by achieving a 100% task completion rate in experimental evaluations, highlighting its effectiveness in handling higher levels of software complexity and offering extensive functionality.\n",
            "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-10-28', 'last_modified_date': '2025-10-28'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Auto-Retrieval Tool"
      ],
      "metadata": {
        "id": "EGMYtivzUIR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def vector_query(\n",
        "    query: str,\n",
        "    page_numbers: List[str]\n",
        ") -> str:\n",
        "    \"\"\"Perform a vector search over an index.\n",
        "\n",
        "    query (str): the string query to be embedded.\n",
        "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "        over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata_dicts = [\n",
        "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "    ]\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filters=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "    response = query_engine.query(query)\n",
        "    return response\n",
        "\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")"
      ],
      "metadata": {
        "id": "a69NkZQDUOYE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGWYH4bBUTgl",
        "outputId": "29aac91d-d332-426b-fb25-424143d6d4a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating a 100% task completion rate in experimental evaluations.\n",
            "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-10-28', 'last_modified_date': '2025-10-28'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add More Tools"
      ],
      "metadata": {
        "id": "bdI82mq4UVAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    name=\"summary_tool\",\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful if you want to get a summary of MetaGPT\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "dFANjpM0UbS1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN6TUTjIUeP7",
        "outputId": "bcb59ce3-836b-4880-ccc5-354c53cecb34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT outperforms ChatDev on the SoftwareDev dataset in various metrics. For example, MetaGPT achieves a higher score in executability, takes less time for execution, requires more tokens but fewer tokens per line of code compared to ChatDev. Additionally, MetaGPT demonstrates autonomous software generation capabilities and highlights the benefits of using SOPs in collaborations between multiple agents.\n",
            "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-10-28', 'last_modified_date': '2025-10-28'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What is a summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLOx8OpkUgDc",
        "outputId": "617f1f07-d5be-4cfd-8db8-8cfcae26e267"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: summary_tool with args: {\"input\": \"Please provide a summary of the paper.\"}\n",
            "=== Function Output ===\n",
            "The paper introduces MetaGPT, a meta-programming framework that enhances multi-agent systems based on Large Language Models (LLMs) through Standardized Operating Procedures (SOPs). It incorporates role specialization, workflow management, and efficient communication mechanisms to improve problem-solving capabilities. MetaGPT utilizes an executable feedback mechanism to enhance code generation quality during runtime and achieves state-of-the-art performance on various benchmarks. The framework facilitates natural language programming by transforming abstract requirements into detailed class and function designs, generating code for tasks like creating games, data processing, and developing GUI applications. It involves various agents like Architects, Engineers, and QA Engineers to ensure high-quality software output. The paper also discusses challenges such as information overload and code hallucinations, emphasizing the importance of clear requirements and structured communication. Additionally, ethical concerns like skill obsolescence and privacy are addressed, highlighting MetaGPT's transparency and accountability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 3: Building an Agent Reasoning Loop"
      ],
      "metadata": {
        "id": "hsuAfoQQUnXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Function Calling Agent"
      ],
      "metadata": {
        "id": "SbG-KyJPUt9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "sMKPrg-IUqDi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[vector_tool, summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "CrClHpeJUzav"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For FunctionAgent - must use run() with await\n",
        "response = await agent.run(\n",
        "    \"Tell me about the agent roles in MetaGPT, and how they communicate.\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr-IbHktU1P6",
        "outputId": "ffa7c75a-9a06-492d-bf36-0ce625739cb0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "In MetaGPT, the agent roles include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. These agents communicate through a shared message pool where they publish structured messages and subscribe to relevant messages based on their profiles. This communication protocol enables agents to exchange information transparently and access necessary data without direct inquiries or responses from other agents. Additionally, agents use a publish-subscribe mechanism to manage information overload and extract task-related information based on their role profiles, ensuring effective communication within the multi-agent system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await  agent.run(\n",
        "    \"Tell me about the evaluation datasets used.\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJpxzzjfU37G",
        "outputId": "7ba8eaf8-60e1-4ac9-9661-1e61c78d83eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "The evaluation datasets used are HumanEval, MBPP, and SoftwareDev.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.run(\"Tell me the results over one of the above datasets.\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "qOZowgzOU6Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5cea53-0019-4347-da55-653478abdccf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "The results over the SoftwareDev dataset show that MetaGPT achieved an average score of 3.9, outperforming ChatDev's score of 2.1. Other general intelligent algorithms like AutoGPT scored 1.0, indicating a lower performance in generating executable code effectively. Models such as AutoGPT, Langchain, and Agent-Verse demonstrate strong problem-solving capabilities but lack the systematic deconstruction of requirements that MetaGPT offers. MetaGPT's structured messaging and feedback mechanisms enhance communication and improve code execution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4: Building a Multi-Document Agent"
      ],
      "metadata": {
        "id": "tBRDERx3WiPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup an Agent Over 3 Papers"
      ],
      "metadata": {
        "id": "oC-ZUdZqWlkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "9p_5UBCMWlAt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"{paper}\""
      ],
      "metadata": {
        "id": "8nCY_iZXWq3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc631d1-1543-413f-fcc2-e7de21fde836"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 17:27:39--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  56.3MB/s    in 0.3s    \n",
            "\n",
            "2025-10-28 17:27:40 (56.3 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n",
            "--2025-10-28 17:27:40--  https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168720 (1.1M) [application/pdf]\n",
            "Saving to: ‘longlora.pdf’\n",
            "\n",
            "longlora.pdf        100%[===================>]   1.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-28 17:27:40 (10.3 MB/s) - ‘longlora.pdf’ saved [1168720/1168720]\n",
            "\n",
            "--2025-10-28 17:27:40--  https://openreview.net/pdf?id=hSyW5go0v8\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244749 (1.2M) [application/pdf]\n",
            "Saving to: ‘selfrag.pdf’\n",
            "\n",
            "selfrag.pdf         100%[===================>]   1.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-28 17:27:41 (10.8 MB/s) - ‘selfrag.pdf’ saved [1244749/1244749]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to create tools for each paper\n",
        "from pathlib import Path\n",
        "\n",
        "def get_doc_tools(file_path: str, name: str):\n",
        "    \"\"\"Get vector and summary query engine tools from a document.\"\"\"\n",
        "\n",
        "    # Load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Create indices\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "\n",
        "    # Create query engines\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "\n",
        "    # Create tools\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=f\"Useful for retrieving specific context from {name}.\",\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=f\"Useful for summarization questions related to {name}.\",\n",
        "    )\n",
        "\n",
        "    return vector_tool, summary_tool"
      ],
      "metadata": {
        "id": "W_PZDk3lWtET"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ],
      "metadata": {
        "id": "EHKWRYVGWunE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0019b4-e82d-43b5-8152-18de05818cc5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "xZRNhKIhWvcr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "print(f\"Number of tools: {len(initial_tools)}\")"
      ],
      "metadata": {
        "id": "i1CKgcR_WyBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d6d1e2-72fb-4b0c-e84a-0e1dc65c82e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tools: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaIndex >= 0.14.6\n",
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "# If any items in `initial_tools` are plain Python functions, wrap them first:\n",
        "# from llama_index.core.tools import FunctionTool\n",
        "# initial_tools = [FunctionTool.from_defaults(fn) for fn in initial_tools]\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True,  # optional: shows workflow logs\n",
        ")\n"
      ],
      "metadata": {
        "id": "nrotN19WWzv8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "        \"and then tell me about the evaluation results\"\n",
        "    )\n",
        "))\n",
        "print(str(response))\n"
      ],
      "metadata": {
        "id": "vbx2Xa5YW6At",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842b5dbf-8766-49b2-aedf-4c2d9121d0dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "The evaluation dataset used in LongLoRA is KILT. \n",
            "\n",
            "The evaluation results of LongLoRA suggest that the model's predictions are frequently plausible and well-supported by relevant passages, particularly excelling in short-form tasks. Human annotators noted that the model's outputs generally align with the assessments of ISREL and ISSUP reflection tokens. The model's performance is assessed based on its capacity to produce precise and informative responses, emphasizing relevance, supportiveness, and overall utility in task completion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ],
      "metadata": {
        "id": "A7YAEWvXW8X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503250ca-3c48-4e77-db34-eacf144f378c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It trains a single LM to retrieve, generate, and critique text passages and its own generation using reflection tokens. This framework allows the LM to adjust retrieval frequency and customize behaviors based on task requirements during the inference phase. Self-RAG outperforms existing models on various tasks, showing improvements in factuality, correctness, and fluency. The model focuses on fine-grained evaluation of text generation, emphasizing factual precision, relevance, retrieval necessity, evidence support, and overall response usefulness. Trained with a Critic LM and a Generator LM, Self-RAG's outputs are often plausible, supported by relevant evidence, and align well with human assessments. The model excels in generating factually accurate responses supported by evidence, with performance improving with larger training data sizes.\n",
            "\n",
            "LongLoRA is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It utilizes special reflection tokens to train a language model to retrieve, generate, and critique text passages and its own outputs. The framework allows for customization of language model behaviors during testing, leading to improved model performance and factuality. LongLoRA focuses on fine-grained evaluation of factual precision in long-form text generation, aiming to ensure that the generated content is factually accurate, relevant, and aligned with given instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup an Agent Over 11 Papers"
      ],
      "metadata": {
        "id": "0jGd-H9zW95t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ],
      "metadata": {
        "id": "HZTLBxFUXBX5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"{paper}\""
      ],
      "metadata": {
        "id": "TCo2FY1jXESb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c92fa6-bd24-4dd6-81d9-a7b97dff31ae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 17:28:09--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  10.9MB/s    in 1.5s    \n",
            "\n",
            "2025-10-28 17:28:11 (10.9 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n",
            "--2025-10-28 17:28:11--  https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168720 (1.1M) [application/pdf]\n",
            "Saving to: ‘longlora.pdf’\n",
            "\n",
            "longlora.pdf        100%[===================>]   1.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-28 17:28:12 (7.47 MB/s) - ‘longlora.pdf’ saved [1168720/1168720]\n",
            "\n",
            "--2025-10-28 17:28:12--  https://openreview.net/pdf?id=LzPWWPAdY4\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 366134 (358K) [application/pdf]\n",
            "Saving to: ‘loftq.pdf’\n",
            "\n",
            "loftq.pdf           100%[===================>] 357.55K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-10-28 17:28:12 (5.44 MB/s) - ‘loftq.pdf’ saved [366134/366134]\n",
            "\n",
            "--2025-10-28 17:28:12--  https://openreview.net/pdf?id=VTF8yNQM66\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2680380 (2.6M) [application/pdf]\n",
            "Saving to: ‘swebench.pdf’\n",
            "\n",
            "swebench.pdf        100%[===================>]   2.56M  9.26MB/s    in 0.3s    \n",
            "\n",
            "2025-10-28 17:28:12 (9.26 MB/s) - ‘swebench.pdf’ saved [2680380/2680380]\n",
            "\n",
            "--2025-10-28 17:28:12--  https://openreview.net/pdf?id=hSyW5go0v8\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244749 (1.2M) [application/pdf]\n",
            "Saving to: ‘selfrag.pdf’\n",
            "\n",
            "selfrag.pdf         100%[===================>]   1.19M  7.46MB/s    in 0.2s    \n",
            "\n",
            "2025-10-28 17:28:13 (7.46 MB/s) - ‘selfrag.pdf’ saved [1244749/1244749]\n",
            "\n",
            "--2025-10-28 17:28:13--  https://openreview.net/pdf?id=9WD9KwssyT\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511626 (500K) [application/pdf]\n",
            "Saving to: ‘zipformer.pdf’\n",
            "\n",
            "zipformer.pdf       100%[===================>] 499.63K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-10-28 17:28:13 (5.71 MB/s) - ‘zipformer.pdf’ saved [511626/511626]\n",
            "\n",
            "--2025-10-28 17:28:13--  https://openreview.net/pdf?id=yV6fD7LYkF\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4171982 (4.0M) [application/pdf]\n",
            "Saving to: ‘values.pdf’\n",
            "\n",
            "values.pdf          100%[===================>]   3.98M  10.4MB/s    in 0.4s    \n",
            "\n",
            "2025-10-28 17:28:14 (10.4 MB/s) - ‘values.pdf’ saved [4171982/4171982]\n",
            "\n",
            "--2025-10-28 17:28:14--  https://openreview.net/pdf?id=hnrB5YHoYu\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34710410 (33M) [application/pdf]\n",
            "Saving to: ‘finetune_fair_diffusion.pdf’\n",
            "\n",
            "finetune_fair_diffu 100%[===================>]  33.10M  29.5MB/s    in 1.1s    \n",
            "\n",
            "2025-10-28 17:28:15 (29.5 MB/s) - ‘finetune_fair_diffusion.pdf’ saved [34710410/34710410]\n",
            "\n",
            "--2025-10-28 17:28:15--  https://openreview.net/pdf?id=WbWtOYIzIK\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 877083 (857K) [application/pdf]\n",
            "Saving to: ‘knowledge_card.pdf’\n",
            "\n",
            "knowledge_card.pdf  100%[===================>] 856.53K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-28 17:28:16 (6.79 MB/s) - ‘knowledge_card.pdf’ saved [877083/877083]\n",
            "\n",
            "--2025-10-28 17:28:16--  https://openreview.net/pdf?id=c5pwL0Soay\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4775879 (4.6M) [application/pdf]\n",
            "Saving to: ‘metra.pdf’\n",
            "\n",
            "metra.pdf           100%[===================>]   4.55M  15.6MB/s    in 0.3s    \n",
            "\n",
            "2025-10-28 17:28:16 (15.6 MB/s) - ‘metra.pdf’ saved [4775879/4775879]\n",
            "\n",
            "--2025-10-28 17:28:16--  https://openreview.net/pdf?id=TpD2aG1h0D\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1973959 (1.9M) [application/pdf]\n",
            "Saving to: ‘vr_mcl.pdf’\n",
            "\n",
            "vr_mcl.pdf          100%[===================>]   1.88M  11.0MB/s    in 0.2s    \n",
            "\n",
            "2025-10-28 17:28:17 (11.0 MB/s) - ‘vr_mcl.pdf’ saved [1973959/1973959]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    try:\n",
        "        vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "    except UnicodeEncodeError as e:\n",
        "        print(f\" Unicode error while processing {paper}: {e}\")\n",
        "        try:\n",
        "            # Attempt to re-read text safely and re-generate tools\n",
        "            text_bytes = Path(paper).read_bytes()\n",
        "            safe_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "            # Optionally, save the cleaned text for inspection\n",
        "            clean_path = Path(paper).with_name(Path(paper).stem + \"_clean.txt\")\n",
        "            clean_path.write_text(safe_text, encoding=\"utf-8\")\n",
        "            print(f\"Saved cleaned version: {clean_path}\")\n",
        "\n",
        "            # Retry tool creation if your get_doc_tools can accept a string path\n",
        "            vector_tool, summary_tool = get_doc_tools(clean_path, Path(paper).stem)\n",
        "            paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "            print(f\" Retried successfully for {paper}\")\n",
        "\n",
        "        except Exception as inner_e:\n",
        "            print(f\" Still failed on {paper}: {inner_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Unexpected error for {paper}: {e}\")\n"
      ],
      "metadata": {
        "id": "D2YQ6MCLXGjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff70cf5-19ee-42fb-8b6f-f45e0b495097"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n",
            "Getting tools for paper: metra.pdf\n",
            "Getting tools for paper: vr_mcl.pdf\n",
            " Unicode error while processing vr_mcl.pdf: 'utf-8' codec can't encode character '\\ud835' in position 94129: surrogates not allowed\n",
            "Saved cleaned version: vr_mcl_clean.txt\n",
            " Retried successfully for vr_mcl.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extend the Agent with Tool Retrieval"
      ],
      "metadata": {
        "id": "EJ4I4TvuXJ5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "0ydbcE-RXMi7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ],
      "metadata": {
        "id": "3zTX2YhlXOlz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "RD46kQ4aXRAm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")\n",
        "tools[2].metadata"
      ],
      "metadata": {
        "id": "2EJybydrXSzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4b6950-a3cd-465b-bc72-770fd4b3ea55"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to vr_mcl.', name='query_engine_tool', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "from llama_index.core.tools import RetrieverTool  # ✅ wrap retrievers as tools\n",
        "\n",
        "retriever_tool = RetrieverTool.from_defaults(\n",
        "    retriever=obj_retriever,\n",
        "    name=\"paper_retriever\",\n",
        "    description=\"Retrieve relevant chunks from the loaded papers.\"\n",
        ")\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[retriever_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=(\n",
        "        \"You are an agent designed to answer queries over a set of given papers. \"\n",
        "        \"Always use the provided tools to answer a question. Do not rely on prior knowledge.\"\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JjX1Z260XVCu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwRmYl81ZLD",
        "outputId": "f1016d2a-5c89-499a-e56a-05ae95ef6eaf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "I apologize for the inconvenience, but it seems I am unable to retrieve information about Self-RAG and LongLoRA at the moment. If you have any other questions or need assistance with something else, please feel free to ask.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Tell me about the evaluation dataset used \"\n",
        "            \"in MetaGPT and compare it against SWE-Bench.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ],
      "metadata": {
        "id": "8KBGrr4lXXF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4ca211-9c41-42ff-a700-4bd4e486b88a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "I apologize for the inconvenience, but I am unable to retrieve information about the evaluation datasets used in MetaGPT and SWE-Bench at the moment. If you have any other questions or need assistance with something else, please feel free to let me know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "            \"Analyze the approach in each paper first.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ],
      "metadata": {
        "id": "RzhrevyqXaBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a00de8-b201-4541-aaf0-5f1d87490a05"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced no event\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "I apologize for the inconvenience, but it seems I am unable to retrieve the papers for LongLoRA and LoftQ at the moment. If you have any specific information or details you would like me to compare and contrast between the two papers, please let me know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End of Notebook**\n",
        "\n",
        "This complete notebook covers all 4 lessons for building agentic RAG systems with LlamaIndex:\n",
        "\n",
        "\n",
        "*   Router Engine - Route queries to appropriate tools\n",
        "\n",
        "*   Tool Calling - Create and use custom function tools\n",
        "*   Agent Reasoning Loop - Build agents with multi-step reasoning\n",
        "*   Multi-Document Agent - Scale to multiple documents with tool retrieval"
      ],
      "metadata": {
        "id": "A3GB4XZHXd4m"
      }
    }
  ]
}