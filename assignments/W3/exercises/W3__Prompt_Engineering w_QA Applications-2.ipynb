{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969S__pMmA4o"
      },
      "source": [
        "# Prompt Engineering Activity\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "Before starting, make sure you have:\n",
        "- **HuggingFace Token**: Get from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "- **Google Colab Account** (Free tier works!) - *Optional, can run locally too*\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Quick Links\n",
        "\n",
        "**üîó Open in Colab**: [Click here](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)\n",
        "\n",
        "**üìÇ View on GitHub**: [Click here](https://github.com/oviya-raja/ist-402-assignments/blob/main/IST402/assignments/W3/reference/W3__Prompt_Engineering%20w_QA%20Applications-2.ipynb)\n",
        "\n",
        "> **‚ö†Ô∏è Note**: Colab link requires the repository to be public on GitHub. If you get a 404 error, see troubleshooting below.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Setup Instructions\n",
        "\n",
        "### Option 1: Google Colab (Recommended for GPU)\n",
        "\n",
        "#### Step 1: Open Notebook\n",
        "- **Method A**: Click the \"Open in Colab\" link above\n",
        "- **Method B**:\n",
        "  1. Go to [Google Colab](https://colab.research.google.com/)\n",
        "  2. Click **File** ‚Üí **Open notebook** ‚Üí **GitHub** tab\n",
        "  3. Enter: `oviya-raja/ist-402-assignments`\n",
        "  4. Navigate to: `assignments/W3/exercises/W3__Prompt_Engineering w_QA Applications-2.ipynb`\n",
        "\n",
        "#### Step 2: Enable GPU (Recommended)\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **GPU** ‚Üí **Save**\n",
        "3. **Runtime** ‚Üí **Restart runtime**\n",
        "\n",
        "#### Step 3: Set Up Token (See Token Setup section below)\n",
        "\n",
        "---\n",
        "\n",
        "## üîê Token Setup\n",
        "\n",
        "### For Google Colab Users\n",
        "\n",
        "**Recommended Method: Using .env file**\n",
        "1. Run **Cell 4** ‚Üí It will automatically create a `.env` file\n",
        "2. Click the **folder icon (üìÅ)** in the left sidebar\n",
        "3. Find and click `.env` file\n",
        "4. Replace `your_token_here` with your actual token\n",
        "5. Save (Ctrl+S or Cmd+S)\n",
        "6. Re-run **Cell 4** ‚Üí Token loaded! ‚úÖ\n",
        "\n",
        "**Quick Method: Direct environment variable**\n",
        "Run this in a new cell before Cell 4:\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_actual_token_here\"\n",
        "```\n",
        "\n",
        "**Get your token**: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "> **üìñ Learn more**: See `ENV_IN_COLAB.md` for detailed explanation of how `.env` files work in Colab\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Troubleshooting\n",
        "\n",
        "### 404 Error When Opening from GitHub\n",
        "\n",
        "**Possible causes:**\n",
        "- Repository doesn't exist yet ‚Üí Use **Option 2** (Upload to Colab)\n",
        "- Repository is private ‚Üí Make it public or use **Option 2**\n",
        "- Wrong branch ‚Üí Try changing `main` to `master` in the link\n",
        "\n",
        "**Solution**: Upload the notebook directly to Colab:\n",
        "1. Download this notebook\n",
        "2. Go to [Google Colab](https://colab.research.google.com/)\n",
        "3. Click **File** ‚Üí **Upload notebook**\n",
        "4. Select the downloaded file\n",
        "\n",
        "### GPU Not Detected in Colab\n",
        "\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **GPU** ‚Üí **Save**\n",
        "3. **Runtime** ‚Üí **Restart runtime**\n",
        "4. Re-run Cell 1 to verify\n",
        "\n",
        "### Token Not Loading\n",
        "\n",
        "- Check that `.env` file exists and has correct format: `HUGGINGFACE_HUB_TOKEN=token` (no spaces)\n",
        "- Make sure you re-ran Cell 4 after creating/editing `.env`\n",
        "- Verify token is valid at [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLmk0WQZn4hO"
      },
      "outputs": [],
      "source": [
        "# Google Colab Setup Verification\n",
        "# Run this cell FIRST to check if everything is set up correctly\n",
        "\n",
        "import sys\n",
        "print(\"üîç Checking Google Colab environment...\")\n",
        "print(f\"   Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"   ‚úÖ Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"   ‚ö†Ô∏è  Not running in Google Colab (local environment)\")\n",
        "\n",
        "# Check GPU availability\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  GPU NOT detected\")\n",
        "        if IN_COLAB:\n",
        "            print(\"   üí° TIP: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
        "            print(\"   üí° Then: Runtime ‚Üí Restart runtime\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  PyTorch not installed yet (will be installed in next cell)\")\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"   1. If GPU not detected in Colab: Enable GPU runtime and restart\")\n",
        "print(\"   2. Run Cell 2: Install packages\")\n",
        "print(\"   3. Run Cell 3: Set up Hugging Face token\")\n",
        "print(\"   4. Continue with remaining cells\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q16cFgD3MfHJ"
      },
      "outputs": [],
      "source": [
        "# Install required packages - run this cell first\n",
        "# Note: FAISS package will be installed conditionally based on GPU availability in Cell 3\n",
        "\n",
        "# Core packages (always needed)\n",
        "%pip install transformers torch sentence-transformers datasets python-dotenv faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKxDf9PYn4hP"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HUGGINGFACE_HUB_TOKEN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-cfpUb2oq7e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nXMPHypoVUUS"
      },
      "outputs": [],
      "source": [
        "# Import libraries we need\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYOyg2_clJAL"
      },
      "outputs": [],
      "source": [
        "# Automatically detect and configure device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaVmk44xMeEY"
      },
      "outputs": [],
      "source": [
        "# Specify which Mistral model to use from Hugging Face\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Define device info\n",
        "if device == \"cuda\":\n",
        "    device_info = torch.cuda.get_device_name(0)\n",
        "else:\n",
        "    device_info = \"CPU\"\n",
        "\n",
        "# Automatically choose dtype\n",
        "if device == \"cuda\":\n",
        "    torch_dtype = torch.bfloat16      # Faster + supported on A100\n",
        "    max_new_tokens = 512\n",
        "else:\n",
        "    torch_dtype = torch.float32\n",
        "    max_new_tokens = 256\n",
        "\n",
        "print(f\"\\n‚è≥ Loading Mistral-7B model...\")\n",
        "print(f\"   Device: {device} ({device_info})\")\n",
        "if device == \"cpu\":\n",
        "    print(\"   ‚è±Ô∏è  Expected load time: 5-15 minutes\")\n",
        "    print(\"   ‚è±Ô∏è  Expected generation: 30-60 seconds per response\")\n",
        "else:\n",
        "    print(\"   ‚è±Ô∏è  Expected load time: 1-2 minutes\")\n",
        "    print(\"   ‚è±Ô∏è  Expected generation: 2-5 seconds per response\")\n",
        "print(\"   üì¶ Model size: ~14GB (will download on first run)\")\n",
        "\n",
        "# Hugging Face Token\n",
        "hf_token = userdata.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "\n",
        "# Conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "# Load the model pipeline\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    token=hf_token,\n",
        "    dtype=torch_dtype,\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=2,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded! Generating response...\")\n",
        "if device == \"cpu\":\n",
        "    print(\"   ‚è±Ô∏è  This may take 30-60 seconds on CPU...\")\n",
        "else:\n",
        "    print(\"   ‚è±Ô∏è  This should take 2-5 seconds on GPU...\")\n",
        "\n",
        "# Generate response\n",
        "import time\n",
        "start_time = time.time()\n",
        "result = chatbot(messages)\n",
        "generation_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Response generated in {generation_time:.2f} seconds\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(result)\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH6ue1EsX09S"
      },
      "outputs": [],
      "source": [
        "# Generate the response and store the full result\n",
        "result = chatbot(messages)\n",
        "\n",
        "# Extract just the assistant's response from the complex output structure\n",
        "# result[0] gets the first (and only) generated sequence\n",
        "# [\"generated_text\"] gets the conversation history with the new response\n",
        "# [-1] gets the last message in the conversation (the assistant's reply)\n",
        "# [\"content\"] gets just the text content without the role information\n",
        "assistant_reply = result[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "# Print only the clean assistant response (without all the extra structure)\n",
        "print(assistant_reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYm3yfvMcQc"
      },
      "outputs": [],
      "source": [
        "# Specify the Mistral model we want to use\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Load the tokenizer (converts text to numbers that the model understands)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "# Load the actual model with device-optimized settings\n",
        "# torch_dtype is automatically set in Cell 4: bfloat16 (GPU) or float32 (CPU)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,                    # Which model to load\n",
        "    token=hf_token,             # Authentication token\n",
        "    dtype=torch.bfloat16,       # Use 16-bit precision for faster processing\n",
        "    device_map=\"auto\"           # Automatically use GPU if available\n",
        ")\n",
        "\n",
        "# Create a simple conversation (just user input, no system prompt this time)\n",
        "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
        "\n",
        "# Convert the conversation into the format the model expects\n",
        "# This applies the model's chat template and converts to tensors\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    conversation,                # The conversation to format\n",
        "    add_generation_prompt=True,  # Add prompt to signal the model should respond\n",
        "    return_dict=True,           # Return as dictionary\n",
        "    return_tensors=\"pt\",        # Return as PyTorch tensors\n",
        ").to(model.device)             # Move to same device as model (GPU/CPU)\n",
        "\n",
        "# Generate the response using the model directly\n",
        "outputs = model.generate(\n",
        "    **inputs,                           # Pass all the formatted inputs\n",
        "    max_new_tokens=1000,               # Maximum length of response\n",
        "    pad_token_id=tokenizer.eos_token_id # Token to use for padding\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMrFI6mVSJ6R"
      },
      "outputs": [],
      "source": [
        "# Print the raw model output tensor (this shows token IDs/numbers, not readable text yet)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhdQol6sUCOH"
      },
      "outputs": [],
      "source": [
        "# Convert the token IDs back to readable text and print the result\n",
        "# outputs[0] gets the first generated sequence, skip_special_tokens removes formatting tokens\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
