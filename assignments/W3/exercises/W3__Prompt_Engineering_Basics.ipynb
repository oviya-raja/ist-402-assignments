{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/assignments/W3/exercises/W3__Prompt_Engineering_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<button onclick=\"Jupyter.notebook.clear_all_output()\" style=\"background-color:#4CAF50;color:white;padding:10px 20px;border:none;border-radius:5px;cursor:pointer;font-size:14px;margin-top:10px;\">üßπ Clear All Outputs</button>\n",
    "\n",
    "<script>\n",
    "// For Jupyter Notebook (local)\n",
    "if (typeof Jupyter !== 'undefined') {\n",
    "    console.log('Jupyter environment detected');\n",
    "} \n",
    "// For Google Colab\n",
    "else if (typeof google !== 'undefined' && google.colab) {\n",
    "    console.log('Colab environment detected');\n",
    "    // Colab uses different API - button will work via Python\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "969S__pMmA4o"
   },
   "source": [
    "# Prompt Engineering Basics\n",
    "\n",
    "**IST402 - AI Agents & RAG Systems**\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color:#e8f5e9;padding:12px;border-left:4px solid #4CAF50;border-radius:5px;margin-bottom:20px;\">\n",
    "<strong>üßπ Quick Actions:</strong> Run <strong>Cell 2</strong> below to clear all outputs if needed!\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What You Need\n",
    "\n",
    "- **HuggingFace Token**: Get from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- **Google Colab** (recommended) or local Python environment\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### Step 1: Open in Colab\n",
    "[Click here to open in Colab](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/assignments/W3/exercises/W3__Prompt_Engineering_Basics.ipynb)\n",
    "\n",
    "**Or manually:**\n",
    "1. Go to [Google Colab](https://colab.research.google.com/)\n",
    "2. **File** ‚Üí **Open notebook** ‚Üí **GitHub** tab\n",
    "3. Enter: `oviya-raja/ist-402-assignments`\n",
    "4. Navigate to: `assignments/W3/exercises/W3__Prompt_Engineering_Basics.ipynb`\n",
    "\n",
    "### Step 2: Enable GPU (Recommended)\n",
    "1. **Runtime** ‚Üí **Change runtime type** ‚Üí Select **GPU** ‚Üí **Save**\n",
    "2. **Runtime** ‚Üí **Restart runtime**\n",
    "\n",
    "### Step 3: Set Up Token\n",
    "**In Colab:**\n",
    "1. Run **Cell 3** (token setup cell)\n",
    "2. Use Colab's `userdata.get('HUGGINGFACE_HUB_TOKEN')` or set environment variable\n",
    "\n",
    "**Locally:**\n",
    "1. Create `.env` file: `HUGGINGFACE_HUB_TOKEN=your_token_here`\n",
    "2. Run the token setup cell\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Getting Started\n",
    "\n",
    "1. Run **Cell 1**: Check environment\n",
    "2. Run **Cell 2**: Install packages\n",
    "3. Run **Cell 3**: Set up token\n",
    "4. Continue with remaining cells in order\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "\n",
    "- **Prompt Engineering**: Creating effective system prompts and user messages\n",
    "- **Pipeline vs Direct Model**: Two ways to interact with AI models\n",
    "- **Device Optimization**: Automatic CPU/GPU configuration\n",
    "- **Class Exercises**: Build business-specific AI assistants\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Start with Cell 1! üéâ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Clear All Outputs\n",
    "# Run this cell to clean up the notebook outputs\n",
    "\n",
    "import os\n",
    "\n",
    "# Check environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üì± Running in Google Colab\")\n",
    "    print(\"\\nüí° To clear all outputs in Colab:\")\n",
    "    print(\"   Go to: Runtime ‚Üí Restart runtime\")\n",
    "    print(\"   Or: Runtime ‚Üí Restart and clear output\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running in Jupyter Notebook\")\n",
    "    try:\n",
    "        from IPython.display import clear_output, display, HTML\n",
    "        # Clear current cell output\n",
    "        clear_output(wait=True)\n",
    "        print(\"‚úÖ Output cleared!\")\n",
    "        print(\"\\nüí° To clear ALL outputs:\")\n",
    "        print(\"   Go to: Kernel ‚Üí Restart & Clear Output\")\n",
    "    except:\n",
    "        print(\"üí° To clear outputs:\")\n",
    "        print(\"   Go to: Kernel ‚Üí Restart & Clear Output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8ASvb8vxen1",
    "outputId": "76639748-b09a-4d8a-a623-1be6ffe62096"
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup Verification\n",
    "# Run this cell FIRST to check if everything is set up correctly\n",
    "\n",
    "import sys\n",
    "print(\"üîç Checking Google Colab environment...\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"   ‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"   ‚ö†Ô∏è  Not running in Google Colab (local environment)\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  GPU NOT detected\")\n",
    "        if IN_COLAB:\n",
    "            print(\"   üí° TIP: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
    "            print(\"   üí° Then: Runtime ‚Üí Restart runtime\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  PyTorch not installed yet (will be installed in next cell)\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. If GPU not detected in Colab: Enable GPU runtime and restart\")\n",
    "print(\"   2. Run Cell 2: Install packages\")\n",
    "print(\"   3. Run Cell 3: Set up Hugging Face token\")\n",
    "print(\"   4. Continue with remaining cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q16cFgD3MfHJ",
    "outputId": "e612c9e4-2e0a-4855-e07c-c13a0222e7d7"
   },
   "outputs": [],
   "source": [
    "# Install required packages - run this cell first\n",
    "# Note: FAISS package will be installed conditionally based on GPU availability in Cell 3\n",
    "\n",
    "# Core packages (always needed)\n",
    "%pip install transformers torch sentence-transformers datasets python-dotenv\n",
    "\n",
    "# FAISS will be installed conditionally in Cell 3 based on device (CPU/GPU)\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-MQ5uhON96Z",
    "outputId": "a11632da-9d19-43a1-cb99-be2d3c55a808"
   },
   "outputs": [],
   "source": [
    "# This cell automatically handles both Colab and local environments\n",
    "\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
    "\n",
    "print(\"‚úÖ Hugging Face token loaded successfully!\")\n",
    "print(f\"   Token preview: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else '****'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUGfRS_CZlF4"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXMPHypoVUUS",
    "outputId": "c08702c7-851a-4125-d5a1-630671b86cf0"
   },
   "outputs": [],
   "source": [
    "# Import libraries we need\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VzBW3FIZlF4"
   },
   "source": [
    "---\n",
    "\n",
    "## üì¶ METHOD 1: Pipeline Approach (The Easy Way)\n",
    "\n",
    "**Think of it like: Using a vending machine**\n",
    "- You put in your request (message)\n",
    "- The machine does everything automatically\n",
    "- You get your result (response)\n",
    "\n",
    "**‚úÖ Pros:** Simple, fast to code, less error-prone  \n",
    "**‚ùå Cons:** Less control, can't customize much  \n",
    "**üéØ Best for:** Learning, quick tests, simple projects\n",
    "\n",
    "**üí° Key Point:** Pipeline = Easy but less control (like using a library function)\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Prompts We're Using:\n",
    "- **System Prompt:** \"You are Tom and I am Jerry\" (sets the AI's role)\n",
    "- **User Prompt:** \"Who are you?\" (the question we're asking)\n",
    "\n",
    "**See the code below to see how simple it is! üëá**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc9EKitbZlF4"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUMHnqTZZlF4"
   },
   "source": [
    "---\n",
    "\n",
    "## üîß METHOD 2: Direct Model Approach (The Detailed Way)\n",
    "\n",
    "**Think of it like: Cooking from scratch**\n",
    "- You prepare ingredients (tokenize text)\n",
    "- You cook step by step (run model)\n",
    "- You plate the food (decode response)\n",
    "\n",
    "**‚úÖ Pros:** Full control, can customize everything  \n",
    "**‚ùå Cons:** More code, more things that can go wrong  \n",
    "**üéØ Best for:** Advanced projects, research, custom needs\n",
    "\n",
    "**üí° Key Point:** Direct = More work but full control (like writing your own function)\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Prompt We're Using:\n",
    "- **User Prompt:** \"What's the weather like in Paris?\" (no system prompt in this example)\n",
    "\n",
    "**See the code below to see each step manually! üëá**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYOyg2_clJAL",
    "outputId": "9cc804f8-8975-407e-bc28-338c840f14a2"
   },
   "outputs": [],
   "source": [
    "# Automatically detect and configure device (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiiLyu9IZlF5"
   },
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary: What You Learned\n",
    "\n",
    "Congratulations! You've now seen both approaches to using AI models:\n",
    "\n",
    "### üì¶ Pipeline Approach (Example 1)\n",
    "- **What it does:** Everything automatically in one function call\n",
    "- **When to use:** Quick prototyping, learning, simple projects\n",
    "- **Key takeaway:** Simple but less control\n",
    "\n",
    "### üîß Direct Model Approach (Example 2)  \n",
    "- **What it does:** Manual control over each step (tokenize ‚Üí generate ‚Üí decode)\n",
    "- **When to use:** Advanced projects, custom needs, research\n",
    "- **Key takeaway:** More work but full control\n",
    "\n",
    "### üí° Remember:\n",
    "- Both approaches use the **same model** - just different ways to interact with it\n",
    "- Pipeline = Easy but less control (like using a library function)\n",
    "- Direct = More work but full control (like writing your own function)\n",
    "\n",
    "**Now you're ready to try the class exercises below!** üéâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850,
     "referenced_widgets": [
      "e45d75682f3943e59da3a67943a7585b",
      "6b9e83798081455eaf2da8608532b3df",
      "b404df9aca3b4b5988339db3598d3914",
      "6e98cc1eec294ce0bae41f3b2efb62a8",
      "bacf3c8ebd8242b9a202387397017e12",
      "df582181c1164a76a2062d834ff6bff4",
      "d655a035058348aeb64fcf77163ad20e",
      "4d6d2de6c1bf4afbbb7e0ca7b75a3bd8",
      "458e8c7959494471937915330668acc9",
      "86f4b679a1f44ae0985a86fe1982ba95",
      "1c3d8bd01ee7493891df0575f96adf0b",
      "f3f483706cfd4721b6a46e31a69f185b",
      "0f49fc98b05e428696b9b533deb2ea3f",
      "ccd0b7ce24514cd1a93be89e72430497",
      "520c951657104ed285e6273a94a94430",
      "a760edbfb557454a95bb3043527dfada",
      "1801595680f04c4cb8bb1fe4ea1436c6",
      "8df755e3b64142a79254a995d6511fc6",
      "774b3f99bdc24a099de9da4a241e4e80",
      "23da3b0538fd46d7be304ddee2b47052",
      "dd735a0641b247e688fd51c515c05b91",
      "0e59e4abdfac46feb0582ba53075b24f",
      "8399b1d2777242e3869fb408e3470c8a",
      "876bf5340344497d840deba4cfe9f58d",
      "3c9f88115fa444c298500926e98ed043",
      "110ccde3dd6343b2a14cecbfa352b672",
      "2c29e68933a846fcb04b2889e322d659",
      "6123cb1d822245118884ffd57bf1cad4",
      "40b6176b2692409892668c93836bfb2b",
      "e518a6493f614ec1ab6f9ef15270c01d",
      "2fab9eeaa7184f569bb483052556452e",
      "d5264c8f664840248689f53496dc77b5",
      "1b5600645c5f49149a08f0dccd0d9112",
      "a53504dc31b446d9ac9bda9abd337ebe",
      "4310fc4ee6674a0faf516a584c363292",
      "34adf2821f2d4096b96d5a434573b93d",
      "292691d1fc2340bba986e577e12e88e7",
      "4cf64ff2f922491dbaf1d3b64da3c237",
      "1f9afabb02854a8a96046f2ccd1f31a2",
      "b1a861d9066248ebb8760a3c5dbb721c",
      "da57e867d78242feb8352f9ae3e12fd0",
      "41ac5a7d531f4967b3486d7eb4de16b8",
      "c5cdebe3d20942c2802047e429fe6186",
      "5c10fd89c71d4ef2bd2243c1b6410635",
      "9d09cc4a30bc484bbc6f74e541cff087",
      "68246b94eb204a78a0fcb494d1b3a165",
      "e9459b71fddc406680567e58e3805a48",
      "4fc60d3e46174620b8991de97a38b1b7",
      "8c23cf024e3241c8a8c92bd6bc01fb1a",
      "fd0369de58f246ca818e37b8c49bb485",
      "f565e009cfc0494b968ab9b007933b97",
      "376c9a669642447f9f1c1bf306055c69",
      "09ce4847dfca41ca94f2d37b67e3c31c",
      "3896729738254ddf94e0d67eebd22654",
      "773662e62905494b85d442440b9f1fc5",
      "14ddfddc14544daab97f7a051dce6c0d",
      "10754874785b4959919f95253541e66a",
      "a642aa4925c64a2f835f44cea3de024e",
      "ea08b38684604fe99392c0209d904fc3",
      "1a320f08822b472d816f28d0ccf1e98f",
      "213dd581f8f24eb6a4b576a6e0d344ca",
      "82ea2c05ff054dfab290caad3837da39",
      "bd2fe8159ec54d3b91a975621a44fca0",
      "8bb3d9ef52b540869abc4607e7207e5d",
      "6e161e9f338a475c834f99193d7797ff",
      "f7ef88ab8f5d4e6d9a445b5b97e29f29",
      "78d020af26874d06a9d602c0d8b8ebff",
      "b6c4ce65b7254af4837370e59f128bce",
      "7c79292716684534bb344f2bceb5df56",
      "543512eb1aaf4c7caa012e85f9cf2ce8",
      "0197d359abbb4cc99633b1a50a3c9122",
      "741770e3d6b04d3e9fa88b33135c66e1",
      "504e6d06aa4041feb8e419c1ab0c84ff",
      "1162d920c6e140b78a6cae915d176b25",
      "e72c25825c164567857b438451e737df",
      "8a6ecd8b427e4855a8e6c07a24829357",
      "d6dda1287ae14735b013b1bfbc36b7dd",
      "f80aa38d7ae142f39a1fe9dfd9feaba9",
      "d36083477a5a4a5eb07252bb010194f1",
      "8522be8f0b80438d94991032d9a391fa",
      "e65961b2b171450494ff23d3e3561fb1",
      "1e39709f66644bbd8272c1e86fa54485",
      "f527d102cfeb40dc9cc68430fde66ce2",
      "cf00a98679284fa5bb14f8cf48191fd1",
      "8eec0321df8a4d0aba83f8d5d3ea833b",
      "26c6091db30e45c2a5116663961d26b5",
      "e6870edbf85b4ab6b2d2d1a2d9a6e059",
      "0f7cacc96fa64c33ad2f4a746f0f0c95",
      "08d5c844123b4571a5b9079252ad3988",
      "2d7e86c1f3514cc3a25e5b9b5ffe5ae3",
      "48ff8c6fa53646219200d0e4344314f2",
      "62f58d6c7e3e44aabc6613fdb1b8c2bf",
      "49f5f793e0604b279f33682ffab1aa80",
      "a26dfa232f744b7fba6e8ff9a6dcbe5e",
      "426ea789030447ca96012d80e4a8d895",
      "665f984572214984880baa4f6228a471",
      "a6326fd61b014fb4b58bd7153ada1dd7",
      "6bebb01dca854ccbb5d7149140ca7667",
      "1e4a1a14b31d49f2a0c3a018adb71c47",
      "0d666908b32446f3a6110748f27c38b8",
      "1bb80d3683564db9bbe362564cb6ddc6",
      "1c36a7fa5eb44595acaf23deb0ed5698",
      "8cd8e14ad3064a1e85e239c419a2e92a",
      "3bb5c58026b14f238f8b8fc39026799a",
      "9565c226c05b44e5ac3a872ae0034698",
      "5541953f07c74d4dbb8194d817cce67d",
      "efaa7550315d43c98990589359decd3a",
      "84b762ff43f84f5fa89cd02517dfd639",
      "6d1f8e24bc694311ace32f8e5eb88b71",
      "0046294f704a49cd9c30c37215073eef",
      "209de9765d1f41bfbbc9b149f9fe0afa",
      "45eda2b00ac14856b4364ee052fbda3d",
      "26294334c73d414eb5a42481da0e9caf",
      "7d3fefab13e24304b6b7b6e80d05b11f",
      "a6184d3b564b47948c14313a5029b80d",
      "667054b1876b4191a77ec352dca47b05",
      "a1557351b9e54735a64dd12545c1425d",
      "5fb7efacd2334d828331c229da644d54",
      "fef66e6deb774fb6b9028e0a7f109415",
      "dd2e905c7777483ea868cb0487c00f48",
      "bd66627779514e1fbcff7a336b5e56d2",
      "767f6d7150a34ddfbf8e775fd9fd99e3",
      "cc8b36722ba5438e9ac81242a668e0ce",
      "aa02e4a300df44a6849fa8e1f3c9b63f",
      "7d4650f052c94a7ca7b9c0d2c554d38e",
      "2f3a293055c3419488b96ffdf9f7baa5",
      "0ed244ff5b664b5799170004673f2763",
      "6979e78a1c3e4ed896a3ac95b7f015e6",
      "3e19d3f3d937403a9fceed8fc7737922",
      "88c44c141a2f407a8aee82a0441bc4a3",
      "f3118c4034884d3b8b11281a30a8a14b",
      "965566a8221d42a794a0449344a40a91"
     ]
    },
    "id": "iaVmk44xMeEY",
    "outputId": "69f3ee7d-72d2-47ea-f485-29fa3617d46f"
   },
   "outputs": [],
   "source": [
    "# Specify which Mistral model to use from Hugging Face\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# ‚ö†Ô∏è PERFORMANCE INFO:\n",
    "# Mistral-7B is a LARGE model (7 billion parameters, ~14GB)\n",
    "# Settings are automatically optimized based on device (CPU/GPU) detected above\n",
    "# The code automatically switches between CPU and GPU optimizations\n",
    "\n",
    "print(f\"\\n‚è≥ Loading Mistral-7B model...\")\n",
    "if device == \"cpu\":\n",
    "    print(f\"   ‚è±Ô∏è  Expected load time: 5-15 minutes\")\n",
    "    print(f\"   ‚è±Ô∏è  Expected generation: 30-60 seconds per response\")\n",
    "\n",
    "    device_info = \"Intel/AMD CPU\"\n",
    "    torch_dtype = torch.float32     # safest for CPUs\n",
    "\n",
    "    max_new_tokens = 256            # reduce memory usage on CPU\n",
    "\n",
    "else:  # GPU\n",
    "    print(f\"   ‚è±Ô∏è  Expected load time: 1-2 minutes\")\n",
    "    print(f\"   ‚è±Ô∏è  Expected generation: 2-5 seconds per response\")\n",
    "\n",
    "    device_info = torch.cuda.get_device_name(0)\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "    max_new_tokens = 512\n",
    "\n",
    "print(f\"   Device: {device} ({device_info})\")\n",
    "print(f\"   Torch: {device} ({torch_dtype})\")\n",
    "print(f\"   üì¶ Model size: ~14GB (will download on first run)\")\n",
    "\n",
    "# Create a conversation with system prompt and user message\n",
    "# System prompt defines the AI's role/personality\n",
    "# User message is what the person is asking\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Tom and I am Jerry\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Set up the text generation pipeline with device-optimized parameters\n",
    "# Settings automatically adapt based on device (CPU/GPU) detected in Cell 4\n",
    "chatbot = pipeline(\n",
    "    \"text-generation\",                              # Task type: generating text\n",
    "    model=model_id,                                 # Which model to use\n",
    "    token=hf_token,                                 # Authentication token for Hugging Face\n",
    "    dtype=torch_dtype,                              # Automatically set: bfloat16 (GPU) or float32 (CPU)\n",
    "    device_map=\"auto\",                              # Automatically use GPU if available\n",
    "    max_new_tokens=max_new_tokens,                  # Automatically set: 512 (GPU) or 256 (CPU)\n",
    "    do_sample=True,                                 # Use random sampling for more creative responses\n",
    "    top_k=10,                                       # Consider top 10 most likely next words\n",
    "    num_return_sequences=1,                         # Generate only 1 response\n",
    "    eos_token_id=2,                                 # Token ID that signals end of response\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded! Generating response...\")\n",
    "if device == \"cpu\":\n",
    "    print(\"   ‚è±Ô∏è  This may take 30-60 seconds on CPU...\")\n",
    "else:\n",
    "    print(\"   ‚è±Ô∏è  This should take 2-5 seconds on GPU...\")\n",
    "\n",
    "# Generate response using the pipeline and print the result\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = chatbot(messages)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Response generated in {generation_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH6ue1EsX09S",
    "outputId": "a2fc690e-6212-4c1b-84c8-02c0eb4d4e16"
   },
   "outputs": [],
   "source": [
    "# Generate the response and store the full result\n",
    "result = chatbot(messages)\n",
    "\n",
    "# Extract just the assistant's response from the complex output structure\n",
    "# result[0] gets the first (and only) generated sequence\n",
    "# [\"generated_text\"] gets the conversation history with the new response\n",
    "# [-1] gets the last message in the conversation (the assistant's reply)\n",
    "# [\"content\"] gets just the text content without the role information\n",
    "assistant_reply = result[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "# Print only the clean assistant response (without all the extra structure)\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "92d9a48b4687434e9247327db8ae5e57",
      "e8496f9edcab485ca4495a7ec61bdbc0",
      "cb9ed8cfcf204884a3211c95895ef4a5",
      "719ac67ffba142349c7042fd6f1d04c3",
      "aeac03e874c2457dae8c83418a0337b1",
      "f84daa91488a41bf8aa9bab2ecfd8984",
      "3a7ecf662bbc4040a13231549df752a2",
      "5800c2339601419cb740f3f8e12d395c",
      "8c2b0cb26b344868a41c8c64d99ec75f",
      "f63603d9acab42d799a6dbd8441d3ab6",
      "8d25c19b03fd41aea08ba869448ea150"
     ]
    },
    "id": "2bYm3yfvMcQc",
    "outputId": "4401f57d-1bca-4ef6-9e5f-8db161cf85e4"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer (converts text to numbers that the model understands)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "# Load the actual model with device-optimized settings\n",
    "# torch_dtype is automatically set in Cell 4: bfloat16 (GPU) or float32 (CPU)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,                    # Which model to load\n",
    "    token=hf_token,             # Authentication token\n",
    "    dtype=torch.bfloat16,       # Use 16-bit precision for faster processing\n",
    "    device_map=\"auto\"           # Automatically use GPU if available\n",
    ")\n",
    "\n",
    "# Create a simple conversation (just user input, no system prompt this time)\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "\n",
    "# Convert the conversation into the format the model expects\n",
    "# This applies the model's chat template and converts to tensors\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    conversation,                # The conversation to format\n",
    "    add_generation_prompt=True,  # Add prompt to signal the model should respond\n",
    "    return_dict=True,           # Return as dictionary\n",
    "    return_tensors=\"pt\",        # Return as PyTorch tensors\n",
    ").to(model.device)             # Move to same device as model (GPU/CPU)\n",
    "\n",
    "# Generate the response using the model directly\n",
    "outputs = model.generate(\n",
    "    **inputs,                           # Pass all the formatted inputs\n",
    "    max_new_tokens=1000,               # Maximum length of response\n",
    "    pad_token_id=tokenizer.eos_token_id # Token to use for padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMrFI6mVSJ6R",
    "outputId": "af1e6698-7f80-473a-f507-7626dcaaf9aa"
   },
   "outputs": [],
   "source": [
    "# Print the raw model output tensor (this shows token IDs/numbers, not readable text yet)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhdQol6sUCOH",
    "outputId": "8628b3fa-7b69-4989-dc81-8cca40b4dbda"
   },
   "outputs": [],
   "source": [
    "# Convert the token IDs back to readable text and print the result\n",
    "# outputs[0] gets the first generated sequence, skip_special_tokens removes formatting tokens\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}