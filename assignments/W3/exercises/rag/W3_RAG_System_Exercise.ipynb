{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Exercise - Building a Complete RAG System with Mistral\n",
    "\n",
    "**IST402 - AI Agents & RAG Systems**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What You Need\n",
    "\n",
    "- **HuggingFace Token**: Get from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- **Google Colab** (recommended) or local Python environment\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### Step 1: Open in Colab\n",
    "[Click here to open in Colab](https://colab.research.google.com/github/oviya-raja/ist-402-assignments/blob/main/assignments/W3/exercises/rag/W3_RAG_System_Exercise.ipynb)\n",
    "\n",
    "üí° **Tip:** Right-click the link above and select \"Open in New Tab\" or use **Ctrl+Click** (Windows/Linux) / **Cmd+Click** (Mac) to open in a new tab.\n",
    "\n",
    "**Or manually:**\n",
    "1. Go to [Google Colab](https://colab.research.google.com/)\n",
    "2. **File** ‚Üí **Open notebook** ‚Üí **GitHub** tab\n",
    "3. Enter: `oviya-raja/ist-402-assignments`\n",
    "4. Navigate to: `assignments/W3/exercises/rag/W3_RAG_System_Exercise.ipynb`\n",
    "\n",
    "### Step 2: Enable GPU (Recommended)\n",
    "1. **Runtime** ‚Üí **Change runtime type** ‚Üí Select **GPU** ‚Üí **Save**\n",
    "2. **Runtime** ‚Üí **Restart runtime**\n",
    "\n",
    "### Step 3: Set Up Token\n",
    "**In Colab:**\n",
    "1. Run the token setup cell\n",
    "2. Use Colab's `userdata.get('HUGGINGFACE_HUB_TOKEN')` or set environment variable\n",
    "\n",
    "**Locally:**\n",
    "1. Create `.env` file: `HUGGINGFACE_HUB_TOKEN=your_token_here`\n",
    "2. Run the token setup cell\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What I Learned\n",
    "\n",
    "This notebook implements a complete RAG (Retrieval-Augmented Generation) system following the class exercise requirements:\n",
    "\n",
    "- **Step 1**: Create business-specific system prompts\n",
    "- **Step 2**: Generate Q&A database using Mistral\n",
    "- **Step 3**: Implement FAISS vector database for similarity search\n",
    "- **Step 4**: Create test questions (answerable and unanswerable)\n",
    "- **Step 5**: Test RAG system performance\n",
    "- **Step 6**: Evaluate and rank multiple QA models\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Start with Cell 1! üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup Verification\n",
    "# Run this cell FIRST to check if everything is set up correctly\n",
    "\n",
    "import sys\n",
    "print(\"üîç Checking Google Colab environment...\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"   ‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"   ‚ö†Ô∏è  Not running in Google Colab (local environment)\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  GPU NOT detected\")\n",
    "        if IN_COLAB:\n",
    "            print(\"   üí° TIP: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
    "            print(\"   üí° Then: Runtime ‚Üí Restart runtime\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  PyTorch not installed yet (will be installed in next cell)\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. If GPU not detected in Colab: Enable GPU runtime and restart\")\n",
    "print(\"   2. Run Cell 4: Install packages\")\n",
    "print(\"   3. Run Cell 5: Set up Hugging Face token\")\n",
    "print(\"   4. Continue with remaining cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - run this cell first\n",
    "# Note: FAISS package will be installed conditionally based on GPU availability\n",
    "\n",
    "# Core packages (always needed)\n",
    "%pip install transformers torch sentence-transformers datasets python-dotenv\n",
    "\n",
    "# FAISS will be installed conditionally based on device (CPU/GPU)\n",
    "%pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell automatically handles both Colab and local environments\n",
    "# IMPORTANT: Run this cell BEFORE executing Step 2!\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
    "    if not hf_token:\n",
    "        raise ValueError(\"Token not found in Colab userdata\")\n",
    "    print(\"‚úÖ Hugging Face token loaded from Colab userdata!\")\n",
    "    print(f\"   Token preview: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else '****'}\")\n",
    "    print(\"\\nüí° Token is now available for all subsequent cells!\")\n",
    "except (ImportError, ValueError):\n",
    "    # Not in Colab or token not in userdata - try environment variable\n",
    "    import os\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "    except ImportError:\n",
    "        pass  # dotenv not installed, continue with os.getenv\n",
    "    \n",
    "    hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    if not hf_token:\n",
    "        print(\"‚ùå Hugging Face token not found!\")\n",
    "        print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
    "        print(\"\\n   In Colab:\")\n",
    "        print(\"   - Use: from google.colab import userdata\")\n",
    "        print(\"   - Then: userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token')\")\n",
    "        print(\"   - Run this cell again\")\n",
    "        print(\"\\n   Locally:\")\n",
    "        print(\"   - Create .env file: HUGGINGFACE_HUB_TOKEN=your_token_here\")\n",
    "        print(\"   - Or set: export HUGGINGFACE_HUB_TOKEN=your_token\")\n",
    "        print(\"   - Run this cell again\")\n",
    "        print(\"\\n‚ö†Ô∏è  You must set up the token before running Step 2!\")\n",
    "    else:\n",
    "        print(\"‚úÖ Hugging Face token loaded from environment!\")\n",
    "        print(f\"   Token preview: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else '****'}\")\n",
    "        print(\"\\nüí° Token is now available for all subsequent cells!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from typing import Dict, Tuple, List, Any, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weights.*not initialized.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*You should probably TRAIN.*\")\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, logging as transformers_logging\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    print(\"‚úÖ All required libraries imported successfully!\")\n",
    "except (ImportError, RuntimeError) as e:\n",
    "    error_msg = str(e)\n",
    "    if \"register_fake\" in error_msg or \"torch.library\" in error_msg:\n",
    "        print(\"‚ùå Dependency version mismatch!\")\n",
    "        print(\"   Fix: pip install --upgrade torch torchvision\")\n",
    "        print(\"   Then restart the kernel and run this cell again.\")\n",
    "        raise ImportError(\"Dependency version mismatch. Please upgrade torch and torchvision.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {error_msg}\")\n",
    "        print(\"   Install with: pip install transformers torch sentence-transformers faiss-cpu\")\n",
    "        print(\"   Then restart the kernel and run this cell again.\")\n",
    "        raise ImportError(f\"Missing required libraries: {error_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "EMBEDDING_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "BUSINESS_NAME = \"TechStart Solutions\"\n",
    "ROLE = \"AI Solutions Consultant\"\n",
    "\n",
    "QA_MODELS = [\n",
    "    \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\",\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "    \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\",\n",
    "    \"gasolsun/DynamicRAG-8B\",\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "    \"mrm8488/bert-tiny-finetuned-squadv2\",\n",
    "]\n",
    "\n",
    "print(f\"üìã Business: {BUSINESS_NAME}\")\n",
    "print(f\"üìã Role: {ROLE}\")\n",
    "print(f\"‚úÖ Configured {len(QA_MODELS)} QA models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is organized step-by-step for clarity and learning. Run all cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection & Configuration\n",
    "\n",
    "Automatically detect if GPU is available and configure accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_availability() -> Tuple[bool, str, str]:\n",
    "    \"\"\"Check if GPU is available.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            return True, torch.cuda.get_device_name(0), torch.version.cuda\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "def create_gpu_config(gpu_name: str, cuda_version: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create GPU configuration dictionary.\"\"\"\n",
    "    return {\n",
    "        \"device\": \"cuda\",\n",
    "        \"device_name\": \"cuda\",\n",
    "        \"gpu_name\": gpu_name,\n",
    "        \"cuda_version\": cuda_version,\n",
    "        \"is_cpu\": False,\n",
    "        \"is_gpu\": True,\n",
    "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"pipeline_device\": 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_cpu_config() -> Dict[str, Any]:\n",
    "    \"\"\"Create CPU configuration dictionary.\"\"\"\n",
    "    return {\n",
    "        \"device\": \"cpu\",\n",
    "        \"device_name\": \"cpu\",\n",
    "        \"gpu_name\": None,\n",
    "        \"cuda_version\": None,\n",
    "        \"is_cpu\": True,\n",
    "        \"is_gpu\": False,\n",
    "        \"torch_dtype\": torch.float32,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"device_map\": None,\n",
    "        \"pipeline_device\": -1,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_device_configuration() -> Dict[str, Any]:\n",
    "    \"\"\"Check device availability and return complete configuration.\"\"\"\n",
    "    print(\"üîç Detecting device (CPU/GPU)...\")\n",
    "    \n",
    "    is_gpu, gpu_name, cuda_version = check_gpu_availability()\n",
    "    \n",
    "    if is_gpu:\n",
    "        print(f\"   ‚úÖ GPU Available: {gpu_name}\")\n",
    "        print(f\"   ‚úÖ CUDA Version: {cuda_version}\")\n",
    "        return create_gpu_config(gpu_name, cuda_version)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  GPU NOT detected - using CPU\")\n",
    "        print(\"   üí° CPU works fine, but GPU is much faster!\")\n",
    "        return create_cpu_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Get Hugging Face token from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_token() -> Optional[str]:\n",
    "    \"\"\"Get Hugging Face token from global scope or environment.\"\"\"\n",
    "    # First check if hf_token is already defined in global scope (from Cell 5)\n",
    "    import __main__\n",
    "    if hasattr(__main__, 'hf_token') and __main__.hf_token:\n",
    "        return __main__.hf_token\n",
    "    \n",
    "    # Try environment variable\n",
    "    token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    if token:\n",
    "        return token\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def setup_token() -> str:\n",
    "    \"\"\"Setup and validate Hugging Face token.\"\"\"\n",
    "    token = get_hf_token()\n",
    "    \n",
    "    if not token:\n",
    "        print(\"\\n‚ùå Hugging Face token not found!\")\n",
    "        print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
    "        print(\"\\n   In Colab:\")\n",
    "        print(\"   - Run Cell 5 first to set up token\")\n",
    "        print(\"   - Or use: from google.colab import userdata\")\n",
    "        print(\"   - Then: userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token')\")\n",
    "        print(\"\\n   Locally:\")\n",
    "        print(\"   - Create .env file: HUGGINGFACE_HUB_TOKEN=your_token_here\")\n",
    "        print(\"   - Or set environment variable: export HUGGINGFACE_HUB_TOKEN=your_token\")\n",
    "        raise ValueError(\"HUGGINGFACE_HUB_TOKEN not found. Please set it up first (see Cell 5).\")\n",
    "    \n",
    "    print(\"‚úÖ Hugging Face token loaded successfully!\")\n",
    "    preview = f\"{token[:10]}...{token[-4:]}\" if len(token) > 14 else \"****\"\n",
    "    print(f\"   Token preview: {preview}\\n\")\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mistral Model\n",
    "\n",
    "Load the Mistral model for generating Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mistral_model(model_id: str, hf_token: str, device_config: Dict[str, Any]) -> Tuple[Any, Any]:\n",
    "    \"\"\"Load Mistral model and tokenizer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Loading Mistral Model\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìö Loading: {model_id}\")\n",
    "    print(f\"‚è≥ This may take 1-2 minutes on first run (downloading model)...\")\n",
    "    print(f\"üí° Note: Verbose warnings are suppressed for cleaner output\\n\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "    \n",
    "    if device_config[\"is_cpu\"]:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            token=hf_token,\n",
    "            dtype=device_config[\"torch_dtype\"],\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).to(\"cpu\")\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            token=hf_token,\n",
    "            dtype=device_config[\"torch_dtype\"],\n",
    "            device_map=device_config[\"device_map\"],\n",
    "        )\n",
    "    \n",
    "    print(\"‚úÖ Mistral model loaded successfully!\\n\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_mistral_pipeline(model: Any, tokenizer: Any, device_config: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Create text generation pipeline from Mistral model.\"\"\"\n",
    "    print(\"Setting up Mistral pipeline...\")\n",
    "    \n",
    "    kwargs = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_new_tokens\": device_config[\"max_new_tokens\"],\n",
    "        \"do_sample\": True,\n",
    "        \"num_return_sequences\": 1,\n",
    "    }\n",
    "    \n",
    "    if device_config[\"device_map\"] is not None:\n",
    "        kwargs[\"device_map\"] = device_config[\"device_map\"]\n",
    "    else:\n",
    "        kwargs[\"device\"] = device_config[\"pipeline_device\"]\n",
    "    \n",
    "    chatbot = pipeline(\"text-generation\", **kwargs)\n",
    "    print(\"‚úÖ Pipeline ready!\\n\")\n",
    "    return chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 1: Create System Prompt\n",
    "\n",
    "Create a system prompt that defines the AI's role and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(business_name: str, role: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a system prompt for the business context.\n",
    "    \n",
    "    Args:\n",
    "        business_name: Name of the business/organization\n",
    "        role: Professional role for the AI (e.g., \"AI Solutions Consultant\")\n",
    "    \n",
    "    Returns:\n",
    "        System prompt string\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Creating System Prompt\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìã Business: {business_name}\")\n",
    "    print(f\"üìã Role: {role}\\n\")\n",
    "    \n",
    "    system_prompt = f\"\"\"You are a {role} at {business_name}. \n",
    "You are knowledgeable, professional, and helpful. \n",
    "You provide accurate information about {business_name}'s services, pricing, processes, and expertise.\n",
    "Always be courteous and aim to help customers understand how {business_name} can assist them.\"\"\"\n",
    "    \n",
    "    print(\"‚úÖ System prompt created:\")\n",
    "    print(f\"   {system_prompt}\\n\")\n",
    "    return system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 1: Create System Prompt\n",
    "system_prompt = create_system_prompt(BUSINESS_NAME, ROLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 2: Generate Q&A Database\n",
    "\n",
    "Use Mistral to generate Q&A pairs (7 answerable + 7 unanswerable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qa_json(response_text: str) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Parse Q&A pairs from JSON response.\n",
    "    \n",
    "    KISS Principle: Simple JSON parsing instead of complex text parsing.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answerable_pairs, unanswerable_pairs)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response (might have markdown code blocks)\n",
    "        response_text = response_text.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        data = json.loads(response_text)\n",
    "        \n",
    "        # Extract answerable and unanswerable pairs\n",
    "        answerable = data.get(\"answerable\", [])\n",
    "        unanswerable = data.get(\"unanswerable\", [])\n",
    "        \n",
    "        # Fallback: if old format (just \"qa_pairs\"), treat all as answerable\n",
    "        if not answerable and not unanswerable and \"qa_pairs\" in data:\n",
    "            answerable = data[\"qa_pairs\"]\n",
    "            unanswerable = []\n",
    "        \n",
    "        return answerable, unanswerable\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: return empty lists if JSON parsing fails\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def generate_qa_database(\n",
    "    chatbot: Any, \n",
    "    system_prompt: str, \n",
    "    business_name: str, \n",
    "    max_retries: int = 2\n",
    ") -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Generate Q&A database using Mistral.\n",
    "    \n",
    "    Args:\n",
    "        chatbot: Mistral pipeline\n",
    "        system_prompt: System prompt for business context\n",
    "        business_name: Name of the business\n",
    "        max_retries: Maximum number of retries if not enough pairs generated\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answerable_pairs, unanswerable_pairs)\n",
    "    \"\"\"\n",
    "    # Generate new Q&A pairs\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 2: Generating Q&A Database\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nüìö Generating 15 question pairs: 7 answerable + 7 unanswerable\")\n",
    "    print(\"   Answerable: Questions the business CAN answer (knowledge base)\")\n",
    "    print(\"   Unanswerable: Questions the business CANNOT answer (outside expertise)\")\n",
    "    print(\"   This may take 30-60 seconds...\\n\")\n",
    "    \n",
    "    # KISS Principle: Request JSON format directly - no parsing needed!\n",
    "    prompt = f\"\"\"Generate 15 question-answer pairs for {business_name}:\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Generate exactly 15 pairs total\n",
    "- 7 answerable pairs: Questions about {business_name}'s services, pricing, processes, technical details, contact info (these go in knowledge base)\n",
    "- 7 unanswerable pairs: Questions about competitor info, unrelated topics, personal details, things outside {business_name}'s expertise (these are for testing)\n",
    "- 1 additional pair (your choice: answerable or unanswerable)\n",
    "\n",
    "Return ONLY valid JSON format (no other text):\n",
    "\n",
    "JSON FORMAT:\n",
    "{{\n",
    "  \"answerable\": [\n",
    "    {{\"question\": \"What services do you offer?\", \"answer\": \"We offer...\"}},\n",
    "    {{\"question\": \"How much does it cost?\", \"answer\": \"Our pricing...\"}}\n",
    "  ],\n",
    "  \"unanswerable\": [\n",
    "    {{\"question\": \"What do your competitors charge?\", \"answer\": \"I don't have information about competitors.\"}},\n",
    "    {{\"question\": \"What's the weather today?\", \"answer\": \"I cannot provide weather information.\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON, nothing else.\"\"\"\n",
    "\n",
    "    answerable_pairs = []\n",
    "    unanswerable_pairs = []\n",
    "    attempts = 0\n",
    "    \n",
    "    while (len(answerable_pairs) < 7 or len(unanswerable_pairs) < 7) and attempts <= max_retries:\n",
    "        attempts += 1\n",
    "        if attempts > 1:\n",
    "            print(f\"   üîÑ Retry attempt {attempts-1}/{max_retries} (need at least 10 pairs)...\\n\")\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # Use longer generation for Q&A pairs (override pipeline's max_new_tokens)\n",
    "            result = chatbot(\n",
    "                messages,\n",
    "                max_new_tokens=1024,  # More tokens for longer output (10-15 Q&A pairs)\n",
    "                do_sample=True,\n",
    "                top_k=10\n",
    "            )\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # Extract response\n",
    "        response_text = result[0][\"generated_text\"][-1][\"content\"]\n",
    "        \n",
    "        # Parse Q&A pairs from JSON (KISS: simple JSON parsing)\n",
    "        answerable, unanswerable = parse_qa_json(response_text)\n",
    "        \n",
    "        answerable_pairs = answerable\n",
    "        unanswerable_pairs = unanswerable\n",
    "        \n",
    "        total_pairs = len(answerable_pairs) + len(unanswerable_pairs)\n",
    "        print(f\"   Attempt {attempts}: Generated {len(answerable_pairs)} answerable + {len(unanswerable_pairs)} unanswerable = {total_pairs} total pairs in {generation_time:.2f} seconds\")\n",
    "        \n",
    "        if len(answerable_pairs) >= 7 and len(unanswerable_pairs) >= 7:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final result:\")\n",
    "    print(f\"   Answerable pairs: {len(answerable_pairs)} (target: 7+)\")\n",
    "    print(f\"   Unanswerable pairs: {len(unanswerable_pairs)} (target: 7+)\")\n",
    "    print(f\"   Total: {len(answerable_pairs) + len(unanswerable_pairs)} pairs\")\n",
    "    \n",
    "    print(\"\\nüìã Sample answerable pairs (knowledge base):\")\n",
    "    for i, qa in enumerate(answerable_pairs[:3], 1):\n",
    "        print(f\"\\n   {i}. Q: {qa['question'][:70]}...\")\n",
    "        print(f\"      A: {qa['answer'][:70]}...\")\n",
    "    \n",
    "    print(\"\\nüìã Sample unanswerable pairs (for testing):\")\n",
    "    for i, qa in enumerate(unanswerable_pairs[:3], 1):\n",
    "        print(f\"\\n   {i}. Q: {qa['question'][:70]}...\")\n",
    "        print(f\"      A: {qa['answer'][:70]}...\")\n",
    "    \n",
    "    if len(answerable_pairs) < 7 or len(unanswerable_pairs) < 7:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Need at least 7 of each type\")\n",
    "        print(f\"   Got: {len(answerable_pairs)} answerable, {len(unanswerable_pairs)} unanswerable\")\n",
    "        print(\"   The script will continue, but you may want to regenerate.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Excellent! Generated {len(answerable_pairs)} answerable + {len(unanswerable_pairs)} unanswerable pairs\")\n",
    "    \n",
    "    print()\n",
    "    return answerable_pairs, unanswerable_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 2: Setup device, token, load Mistral, and generate Q&A\n",
    "# NOTE: Make sure you've run Cell 5 (token setup) first!\n",
    "\n",
    "device_config = get_device_configuration()\n",
    "\n",
    "# Get token (from Cell 5 or environment)\n",
    "try:\n",
    "    # Try to use token from Cell 5 if it exists\n",
    "    if 'hf_token' not in globals() or not hf_token:\n",
    "        hf_token = setup_token()\n",
    "    else:\n",
    "        print(\"‚úÖ Using Hugging Face token from Cell 5\")\n",
    "        preview = f\"{hf_token[:10]}...{hf_token[-4:]}\" if len(hf_token) > 14 else \"****\"\n",
    "        print(f\"   Token preview: {preview}\\n\")\n",
    "except (NameError, ValueError) as e:\n",
    "    print(\"‚ùå Error: Hugging Face token not set up!\")\n",
    "    print(\"   Please run Cell 5 first to set up your token.\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise\n",
    "\n",
    "model, tokenizer = load_mistral_model(MISTRAL_MODEL_ID, hf_token, device_config)\n",
    "chatbot = create_mistral_pipeline(model, tokenizer, device_config)\n",
    "answerable_qa, unanswerable_qa = generate_qa_database(chatbot, system_prompt, BUSINESS_NAME)\n",
    "qa_database = answerable_qa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 3: Implement FAISS Vector Database\n",
    "\n",
    "Convert Q&A database into embeddings and create FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(questions: List[str], embedding_model: Any) -> np.ndarray:\n",
    "    \"\"\"Create embeddings for questions using sentence transformers.\"\"\"\n",
    "    print(\"Creating embeddings for questions...\")\n",
    "    # Suppress progress bar for cleaner output (students can see final results)\n",
    "    embeddings = embedding_model.encode(questions, show_progress_bar=False)\n",
    "    return embeddings.astype('float32')\n",
    "\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"Build FAISS index from embeddings.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def search_similar_questions(\n",
    "    query: str,\n",
    "    embedding_model: Any,\n",
    "    faiss_index: faiss.Index,\n",
    "    qa_database: List[Dict[str, str]],\n",
    "    top_k: int = 3\n",
    ") -> List[Tuple[Dict[str, str], float]]:\n",
    "    \"\"\"\n",
    "    Search for similar questions in the database.\n",
    "    \n",
    "    Returns:\n",
    "        List of (qa_pair, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = query_embedding.astype('float32')\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        if idx < len(qa_database):\n",
    "            # Convert distance to similarity (lower distance = higher similarity)\n",
    "            similarity = 1.0 / (1.0 + dist)\n",
    "            results.append((qa_database[idx], similarity))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def implement_faiss_database(qa_database: List[Dict[str, str]], hf_token: str) -> Tuple[Any, faiss.Index]:\n",
    "    \"\"\"\n",
    "    Implement FAISS vector database.\n",
    "    \n",
    "    Args:\n",
    "        qa_database: List of Q&A pairs\n",
    "        hf_token: Hugging Face token\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (embedding_model, faiss_index)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: Implementing FAISS Vector Database\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìö Loading embedding model: {EMBEDDING_MODEL_ID}\")\n",
    "    print(\"   This converts text to numerical vectors for similarity search...\\n\")\n",
    "    \n",
    "    # Load embedding model\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_ID)\n",
    "    print(\"‚úÖ Embedding model loaded!\\n\")\n",
    "    \n",
    "    # Extract questions\n",
    "    questions = [qa[\"question\"] for qa in qa_database]\n",
    "    print(f\"üìã Creating embeddings for {len(questions)} questions...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = create_embeddings(questions, embedding_model)\n",
    "    print(f\"‚úÖ Created {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}\\n\")\n",
    "    \n",
    "    # Build FAISS index\n",
    "    print(\"üìã Building FAISS index...\")\n",
    "    faiss_index = build_faiss_index(embeddings)\n",
    "    print(f\"‚úÖ FAISS index built with {faiss_index.ntotal} vectors\\n\")\n",
    "    \n",
    "    # Test search\n",
    "    print(\"üß™ Testing search functionality...\")\n",
    "    test_query = questions[0] if questions else \"What services do you offer?\"\n",
    "    results = search_similar_questions(test_query, embedding_model, faiss_index, qa_database, top_k=3)\n",
    "    \n",
    "    print(f\"   Query: {test_query[:50]}...\")\n",
    "    print(\"   Top matches:\")\n",
    "    for i, (qa, similarity) in enumerate(results, 1):\n",
    "        print(f\"   {i}. Similarity: {similarity:.3f} - {qa['question'][:50]}...\")\n",
    "    print()\n",
    "    \n",
    "    return embedding_model, faiss_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 3: Implement FAISS Vector Database\n",
    "embedding_model, faiss_index = implement_faiss_database(qa_database, hf_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 4: Create Test Questions\n",
    "\n",
    "Generate test questions (answerable and unanswerable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_questions(\n",
    "    chatbot: Any,\n",
    "    system_prompt: str,\n",
    "    question_type: str,\n",
    "    business_name: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate test questions using Mistral.\n",
    "    \n",
    "    Args:\n",
    "        question_type: \"answerable\" or \"unanswerable\"\n",
    "    \"\"\"\n",
    "    if question_type == \"answerable\":\n",
    "        prompt = f\"\"\"Generate 5 questions that {business_name} CAN answer about their services, pricing, processes, or expertise.\n",
    "Make them realistic customer questions.\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Generate 5 questions that {business_name} CANNOT answer.\n",
    "These should be about:\n",
    "- Competitor information\n",
    "- Unrelated topics outside their expertise\n",
    "- Personal details\n",
    "- Information not in their knowledge base\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = chatbot(messages)\n",
    "    \n",
    "    response_text = result[0][\"generated_text\"][-1][\"content\"]\n",
    "    \n",
    "    # Parse questions (numbered list or Q: format)\n",
    "    questions = []\n",
    "    for line in response_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        # Remove numbering (1., 2., etc.) or Q: prefix\n",
    "        line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
    "        line = re.sub(r'^Q[\\.:]\\s*', '', line, flags=re.IGNORECASE)\n",
    "        if line and len(line) > 10:  # Valid question\n",
    "            questions.append(line)\n",
    "    \n",
    "    return questions[:5]  # Return up to 5 questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 4: Prepare Test Questions\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: Preparing Test Questions\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìö Using unanswerable pairs from Step 2 as test questions\")\n",
    "print(f\"   Unanswerable test questions: {len(unanswerable_qa)}\")\n",
    "print(\"\\nüìö Generating additional answerable test questions...\")\n",
    "additional_answerable = generate_test_questions(chatbot, system_prompt, \"answerable\", BUSINESS_NAME)\n",
    "answerable = [qa[\"question\"] for qa in answerable_qa[:5]] + additional_answerable[:2]\n",
    "unanswerable = [qa[\"question\"] for qa in unanswerable_qa[:7]]\n",
    "print(f\"   Answerable test questions: {len(answerable)}\")\n",
    "print(f\"   Unanswerable test questions: {len(unanswerable)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 5: Test RAG System\n",
    "\n",
    "Test the RAG system with both question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_system(\n",
    "    questions: List[str],\n",
    "    embedding_model: Any,\n",
    "    faiss_index: faiss.Index,\n",
    "    qa_database: List[Dict[str, str]],\n",
    "    threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Test RAG system with questions and return performance metrics.\"\"\"\n",
    "    results = []\n",
    "    correct = 0\n",
    "    \n",
    "    for question in questions:\n",
    "        search_results = search_similar_questions(\n",
    "            question, embedding_model, faiss_index, qa_database, top_k=1\n",
    "        )\n",
    "        \n",
    "        if search_results:\n",
    "            best_match, similarity = search_results[0]\n",
    "            is_answerable = similarity >= threshold\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"similarity\": similarity,\n",
    "                \"is_answerable\": is_answerable,\n",
    "                \"matched_qa\": best_match\n",
    "            })\n",
    "            if is_answerable:\n",
    "                correct += 1\n",
    "        else:\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"similarity\": 0.0,\n",
    "                \"is_answerable\": False,\n",
    "                \"matched_qa\": None\n",
    "            })\n",
    "    \n",
    "    accuracy = correct / len(questions) if questions else 0.0\n",
    "    avg_similarity = sum(r[\"similarity\"] for r in results) / len(results) if results else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_similarity\": avg_similarity,\n",
    "        \"correct\": correct,\n",
    "        \"total\": len(questions)\n",
    "    }\n",
    "\n",
    "\n",
    "def implement_and_test_questions(\n",
    "    answerable: List[str],\n",
    "    unanswerable: List[str],\n",
    "    embedding_model: Any,\n",
    "    faiss_index: faiss.Index,\n",
    "    qa_database: List[Dict[str, str]]\n",
    ") -> None:\n",
    "    \"\"\"Test RAG system with both question types.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 5: Testing RAG System\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìä Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "    print(\"   (Questions with similarity >= threshold are considered answerable)\\n\")\n",
    "    \n",
    "    # Test answerable questions\n",
    "    print(\"üß™ Testing answerable questions...\")\n",
    "    answerable_results = test_rag_system(\n",
    "        answerable, embedding_model, faiss_index, qa_database\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {answerable_results['accuracy']:.1%} ({answerable_results['correct']}/{answerable_results['total']})\")\n",
    "    print(f\"   üìä Average similarity: {answerable_results['avg_similarity']:.3f}\\n\")\n",
    "    \n",
    "    # Test unanswerable questions\n",
    "    print(\"üß™ Testing unanswerable questions...\")\n",
    "    unanswerable_results = test_rag_system(\n",
    "        unanswerable, embedding_model, faiss_index, qa_database\n",
    "    )\n",
    "    \n",
    "    # For unanswerable, we want LOW similarity (so accuracy = 1 - correct/total)\n",
    "    unanswerable_correct = unanswerable_results['total'] - unanswerable_results['correct']\n",
    "    unanswerable_accuracy = unanswerable_correct / unanswerable_results['total'] if unanswerable_results['total'] > 0 else 0.0\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {unanswerable_accuracy:.1%} ({unanswerable_correct}/{unanswerable_results['total']} correctly identified as unanswerable)\")\n",
    "    print(f\"   üìä Average similarity: {unanswerable_results['avg_similarity']:.3f}\\n\")\n",
    "    \n",
    "    # Overall performance\n",
    "    total_correct = answerable_results['correct'] + unanswerable_correct\n",
    "    total_questions = answerable_results['total'] + unanswerable_results['total']\n",
    "    overall_accuracy = total_correct / total_questions if total_questions > 0 else 0.0\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä Overall Performance Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"   Overall Accuracy: {overall_accuracy:.1%} ({total_correct}/{total_questions})\")\n",
    "    print(f\"   Answerable Questions: {answerable_results['accuracy']:.1%} accuracy\")\n",
    "    print(f\"   Unanswerable Questions: {unanswerable_accuracy:.1%} accuracy\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 5: Test RAG System\n",
    "implement_and_test_questions(answerable, unanswerable, embedding_model, faiss_index, qa_database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 6: Model Experimentation and Ranking\n",
    "\n",
    "Test and rank multiple QA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_model(\n",
    "    model_id: str,\n",
    "    question: str,\n",
    "    context: str,\n",
    "    hf_token: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a single QA model on a question-context pair.\n",
    "    \n",
    "    Why Explicit QA Models Help:\n",
    "    - They return confidence scores (0.0 to 1.0) showing how sure they are\n",
    "    - They're optimized for speed (milliseconds vs seconds)\n",
    "    - They handle edge cases better (unanswerable questions, ambiguous contexts)\n",
    "    - They provide structured output (answer, score, start/end positions)\n",
    "    \n",
    "    General models (like Qwen) require:\n",
    "    - Manual prompt engineering\n",
    "    - No confidence scores\n",
    "    - Slower inference\n",
    "    - Less reliable for QA tasks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Suppress warnings during model loading and inference\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            # Try QA pipeline first (for explicit QA models)\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                qa_pipeline = pipeline(\n",
    "                    \"question-answering\",\n",
    "                    model=model_id,\n",
    "                    token=hf_token\n",
    "                )\n",
    "                load_time = time.time() - start_time\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = qa_pipeline(question=question, context=context)\n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                return {\n",
    "                    \"model_id\": model_id,\n",
    "                    \"answer\": result.get(\"answer\", \"\"),\n",
    "                    \"score\": result.get(\"score\", 0.0),  # Confidence score (QA models provide this!)\n",
    "                    \"load_time\": load_time,\n",
    "                    \"inference_time\": inference_time,\n",
    "                    \"success\": True,\n",
    "                    \"model_type\": \"explicit_qa\"  # Explicit QA model\n",
    "                }\n",
    "            except (ValueError, OSError, RuntimeError) as e:\n",
    "                # Fallback: Try as text generation model (for general models like Qwen/Mistral)\n",
    "                # This shows why explicit QA models are better - they work directly!\n",
    "                try:\n",
    "                    gen_pipeline = pipeline(\n",
    "                        \"text-generation\",\n",
    "                        model=model_id,\n",
    "                        token=hf_token,\n",
    "                        max_new_tokens=50,\n",
    "                        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "                    )\n",
    "                    load_time = time.time() - start_time\n",
    "                    \n",
    "                    # Create prompt for general model\n",
    "                    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "                    start_time = time.time()\n",
    "                    result = gen_pipeline(prompt, return_full_text=False)\n",
    "                    inference_time = time.time() - start_time\n",
    "                    \n",
    "                    answer = result[0][\"generated_text\"].strip()\n",
    "                    \n",
    "                    return {\n",
    "                        \"model_id\": model_id,\n",
    "                        \"answer\": answer,\n",
    "                        \"score\": 0.5,  # No confidence score available (general model limitation)\n",
    "                        \"load_time\": load_time,\n",
    "                        \"inference_time\": inference_time,\n",
    "                        \"success\": True,\n",
    "                        \"model_type\": \"general\"  # General model (not QA-specific)\n",
    "                    }\n",
    "                except Exception as gen_error:\n",
    "                    # Text generation also failed - likely model too large for CPU\n",
    "                    raise Exception(f\"Text generation failed: {str(gen_error)}. Model may be too large for CPU. Try GPU or use a smaller model.\")\n",
    "    except Exception as e:\n",
    "        # Extract error message (truncate if too long)\n",
    "        error_msg = str(e)\n",
    "        if len(error_msg) > 150:\n",
    "            error_msg = error_msg[:147] + \"...\"\n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"answer\": \"\",\n",
    "            \"score\": 0.0,\n",
    "            \"load_time\": 0.0,\n",
    "            \"inference_time\": 0.0,\n",
    "            \"success\": False,\n",
    "            \"error\": error_msg,\n",
    "            \"model_type\": \"unknown\"\n",
    "        }\n",
    "\n",
    "\n",
    "def rank_qa_models(\n",
    "    qa_database: List[Dict[str, str]],\n",
    "    embedding_model: Any,\n",
    "    faiss_index: faiss.Index,\n",
    "    hf_token: str,\n",
    "    test_questions: Optional[List[str]] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Evaluate and rank QA models.\n",
    "    \n",
    "    Evaluation Tasks (from class exercise):\n",
    "    1. Speed: Measure response time (milliseconds)\n",
    "    2. Confidence Scores: How sure is the model? (0.0 to 1.0)\n",
    "    3. Answer Quality: How correct/helpful is the answer?\n",
    "    \n",
    "    Models are tested on questions from the Q&A database.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 6: Model Experimentation and Ranking\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìö Testing {len(QA_MODELS)} QA models...\")\n",
    "    print(\"\\nüìä Evaluation Tasks:\")\n",
    "    print(\"   1. Speed: Response time (milliseconds)\")\n",
    "    print(\"   2. Confidence Scores: Model certainty (0.0 to 1.0)\")\n",
    "    print(\"   3. Answer Quality: Correctness and helpfulness\")\n",
    "    print(\"\\nüí° What You'll See:\")\n",
    "    print(\"   - Each model will be tested on sample questions\")\n",
    "    print(\"   - Results show confidence score and speed\")\n",
    "    print(\"   - Models are ranked by performance\")\n",
    "    print(\"   - Note: Model loading may take time (first time only)\")\n",
    "    print(\"\\n   ‚è≥ This may take several minutes...\\n\")\n",
    "    \n",
    "    # Use test questions if provided, otherwise use Q&A database\n",
    "    if test_questions is None:\n",
    "        test_questions = [qa[\"question\"] for qa in qa_database[:3]]  # Test on 3 questions\n",
    "    \n",
    "    # Build context from answers\n",
    "    context = \" \".join([qa[\"answer\"] for qa in qa_database[:3]])\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for i, model_id in enumerate(QA_MODELS, 1):\n",
    "        # Show progress with clear formatting\n",
    "        model_name = model_id.split(\"/\")[-1] if \"/\" in model_id else model_id\n",
    "        print(f\"   [{i}/{len(QA_MODELS)}] Testing {model_name}...\", end=\" \", flush=True)\n",
    "        \n",
    "        # Test on a single question for evaluation\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            question = test_questions[0]  # Test on first question\n",
    "            result = evaluate_qa_model(model_id, question, context, hf_token)\n",
    "        \n",
    "        model_results.append(result)\n",
    "        \n",
    "        # Show result on same line\n",
    "        if result[\"success\"]:\n",
    "            print(f\"‚úÖ Score: {result['score']:.3f} | Time: {result['inference_time']:.2f}s\")\n",
    "        else:\n",
    "            error_msg = result.get(\"error\", \"Unknown error\")\n",
    "            # Show helpful error message for students\n",
    "            if \"too large\" in error_msg.lower() or \"out of memory\" in error_msg.lower():\n",
    "                print(f\"‚ùå Failed: Model too large for CPU (needs GPU or smaller model)\")\n",
    "            else:\n",
    "                # Truncate long error messages\n",
    "                short_error = error_msg[:60] + \"...\" if len(error_msg) > 60 else error_msg\n",
    "                print(f\"‚ùå Failed: {short_error}\")\n",
    "        print()\n",
    "    \n",
    "    # Rank models by confidence score (primary) and speed (secondary)\n",
    "    model_results.sort(key=lambda x: (x.get(\"score\", 0.0), -x.get(\"inference_time\", float('inf'))), reverse=True)\n",
    "    \n",
    "    # Print ranking as a table\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä Model Ranking - Review and Choose the Best Model for Your Use Case\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"{'Rank':<6} {'Model':<50} {'Type':<15} {'Confidence':<12} {'Speed (s)':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i, result in enumerate(model_results, 1):\n",
    "        model_name = result['model_id'].split('/')[-1] if '/' in result['model_id'] else result['model_id']\n",
    "        if len(model_name) > 48:\n",
    "            model_name = model_name[:45] + \"...\"\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            model_type = result.get(\"model_type\", \"unknown\")\n",
    "            type_label = \"QA-Specific\" if model_type == \"explicit_qa\" else \"General\"\n",
    "            confidence = f\"{result['score']:.3f}\"\n",
    "            speed = f\"{result['inference_time']:.3f}\"\n",
    "        else:\n",
    "            type_label = \"Failed\"\n",
    "            confidence = \"N/A\"\n",
    "            speed = \"N/A\"\n",
    "        \n",
    "        print(f\"{i:<6} {model_name:<50} {type_label:<15} {confidence:<12} {speed:<12}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\nüí° Understanding the Results:\")\n",
    "    print(\"   - Confidence Score: How sure the model is (0.0 = unsure, 1.0 = very sure)\")\n",
    "    print(\"   - Speed: Lower is better (response time in seconds)\")\n",
    "    print(\"   - Model Type: QA-Specific models are optimized for question-answering\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    return model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 6: Model Experimentation and Ranking\n",
    "test_questions = [qa[\"question\"] for qa in answerable_qa[:5]]\n",
    "model_rankings = rank_qa_models(qa_database, embedding_model, faiss_index, hf_token, test_questions=test_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Exercise Completed!\n",
    "\n",
    "You have successfully built a complete RAG system with:\n",
    "- System prompt creation\n",
    "- Q&A database generation\n",
    "- FAISS vector database\n",
    "- Test question generation\n",
    "- RAG system testing\n",
    "- QA model evaluation and ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
