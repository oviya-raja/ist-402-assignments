{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Assignment: RAG-Based Question Answering System with Mistral\n",
        "\n",
        "**Course:** IST402 - AI Agents & RAG Systems  \n",
        "**Student:** [Your Name]  \n",
        "**Date:** [Date]  \n",
        "**Submission:** [Link to completed notebook]\n",
        "\n",
        "---\n",
        "\n",
        "## Assignment Objective\n",
        "\n",
        "Design and implement a **Retrieval-Augmented Generation (RAG)** system using:\n",
        "- Mistral-7B-Instruct-v0.3\n",
        "- FAISS vector database\n",
        "- Custom business data\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Install Required Libraries](#install-libraries)\n",
        "2. [Import Libraries](#import-libraries)\n",
        "3. [Task 1: Create System Prompt](#task-1)\n",
        "4. [Task 2: Generate Business Database](#task-2)\n",
        "5. [Task 3: Implement FAISS Vector Database](#task-3)\n",
        "6. [Task 4: Create Test Questions](#task-4)\n",
        "7. [Task 5: Test Questions](#task-5)\n",
        "8. [Task 6: Model Experimentation & Ranking](#task-6)\n",
        "9. [Reflection & Analysis](#reflection)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Libraries {#install-libraries}\n",
        "\n",
        "Install all necessary packages for the RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/vscode/.local/lib/python3.11/site-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/vscode/.local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain in /home/vscode/.local/lib/python3.11/site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /home/vscode/.local/lib/python3.11/site-packages (from langchain) (1.0.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/vscode/.local/lib/python3.11/site-packages (from langchain) (2.12.4)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.48)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/vscode/.local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/vscode/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain-community in /home/vscode/.local/lib/python3.11/site-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (1.1.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (0.4.48)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community) (2.3.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vscode/.local/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.4)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/vscode/.local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/vscode/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/vscode/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /home/vscode/.local/lib/python3.11/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in /home/vscode/.local/lib/python3.11/site-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\n",
            "Requirement already satisfied: scipy in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /home/vscode/.local/lib/python3.11/site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.11/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/vscode/.local/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: faiss-cpu in /home/vscode/.local/lib/python3.11/site-packages (1.13.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/vscode/.local/lib/python3.11/site-packages (from faiss-cpu) (2.3.5)\n",
            "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in /home/vscode/.local/lib/python3.11/site-packages (0.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in /home/vscode/.local/lib/python3.11/site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (2.9.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
            "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/vscode/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in /home/vscode/.local/lib/python3.11/site-packages (0.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install all required libraries for the RAG system\n",
        "# Each library serves a specific purpose:\n",
        "\n",
        "%pip install transformers          # For pre-trained AI models (BERT, DistilBERT, Mistral, etc.)\n",
        "%pip install langchain             # Framework for building applications with language models\n",
        "%pip install langchain-community   # Community extensions for LangChain\n",
        "%pip install sentence-transformers # For creating text embeddings (converting text to numbers)\n",
        "%pip install torch                 # PyTorch - deep learning framework (backend for transformers)\n",
        "%pip install faiss-cpu            # Facebook AI Similarity Search - for fast similarity searches\n",
        "%pip install sentencepiece         # Required for Mistral tokenizer\n",
        "%pip install accelerate            # For efficient model loading and inference\n",
        "%pip install sentencepiece         # For \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup {#import-libraries}\n",
        "\n",
        "Import all necessary libraries for building the RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all the libraries we need for our RAG system\n",
        "\n",
        "# Import pipeline from transformers - this gives us easy access to pre-trained models\n",
        "from transformers import pipeline\n",
        "\n",
        "# Import FAISS for creating a searchable database of text\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Import embeddings to convert text into numerical vectors for similarity search\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Import Document class to structure our knowledge data\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Import Mistral model for generating content\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Create an Assistant System Prompt {#task-1}\n",
        "\n",
        "**Objective:** Design a system prompt that gives Mistral-7B-Instruct a specific role and business context.\n",
        "\n",
        "---\n",
        "\n",
        "### What is a System Prompt?\n",
        "\n",
        "A **system prompt** is a set of instructions that define the AI's role, behavior, and context before it processes user inputs. Think of it as giving the AI a \"job description\" that shapes how it responds.\n",
        "\n",
        "**Key Characteristics:**\n",
        "- **Role Definition**: Tells the AI what role it should play (e.g., \"You are a marketing expert\")\n",
        "- **Context Setting**: Provides background information about the business/organization\n",
        "- **Behavior Guidance**: Sets expectations for tone, style, and response format\n",
        "- **Constraint Setting**: Defines boundaries and limitations\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "\"You are a customer service representative for an e-commerce platform. \n",
        "You are friendly, professional, and knowledgeable about our products and policies. \n",
        "Always provide accurate information based on our company guidelines.\"\n",
        "```\n",
        "\n",
        "**Why System Prompts Matter:**\n",
        "- They shape the AI's personality and expertise level\n",
        "- They provide context that persists throughout the conversation\n",
        "- They help prevent hallucinations by grounding responses in defined roles\n",
        "- They enable consistent, domain-specific outputs\n",
        "\n",
        "---\n",
        "\n",
        "### What is Mistral-7B-Instruct-v0.3?\n",
        "\n",
        "**Mistral-7B-Instruct-v0.3** is a large language model developed by Mistral AI, specifically optimized for following instructions and generating structured outputs.\n",
        "\n",
        "**Key Features:**\n",
        "- **Model Size**: 7 billion parameters (relatively compact but powerful)\n",
        "- **Type**: Instruction-tuned model (designed to follow prompts and instructions)\n",
        "- **Open Source**: Available on Hugging Face for free use\n",
        "- **Capabilities**: \n",
        "  - Text generation\n",
        "  - Question answering\n",
        "  - Content creation\n",
        "  - Following complex instructions\n",
        "  - Generating structured outputs (like Q&A pairs)\n",
        "\n",
        "**Why Use Mistral-7B-Instruct:**\n",
        "- **Instruction Following**: Specifically trained to follow system prompts and instructions\n",
        "- **Quality Output**: Produces coherent, contextually appropriate responses\n",
        "- **Efficiency**: Smaller than models like GPT-4 but still very capable\n",
        "- **Accessibility**: Free to use via Hugging Face, no API costs\n",
        "- **Flexibility**: Can be fine-tuned for specific tasks\n",
        "\n",
        "**Model Card**: Available at `https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3`\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "- Use `mistralai/Mistral-7B-Instruct-v0.3` to generate your content\n",
        "- Define a specific role (e.g., \"You are a marketing expert for a tech startup\")\n",
        "- Choose a business/organization context to use throughout the assignment\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1: Choose Your Business Context\n",
        "\n",
        "**My Business Context:** [Describe your chosen business/organization here]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Business Context: IST402 - AI Agents & RAG Systems Course\n",
            "Business Role: Week 3 Assignment FAQ Assistant and Concept Explainer\n",
            "\n",
            "üí° This example will help you build an FAQ system for the IST402 course!\n",
            "   You can answer questions about assignments, technologies, and course content.\n"
          ]
        }
      ],
      "source": [
        "# Define your business context\n",
        "# Example Use Case: Building an FAQ RAG system for IST402 course\n",
        "\n",
        "# Simple student-friendly example: IST402 Course FAQ System\n",
        "BUSINESS_CONTEXT = \"IST402 - AI Agents & RAG Systems Course\"\n",
        "BUSINESS_ROLE = \"Week 3 Assignment FAQ Assistant and Concept Explainer\"\n",
        "\n",
        "# Alternative examples you can use:\n",
        "# BUSINESS_CONTEXT = \"Tech Startup - AI Consultant\"\n",
        "# BUSINESS_ROLE = \"AI Consultant\"\n",
        "# \n",
        "# BUSINESS_CONTEXT = \"E-commerce Platform\"\n",
        "# BUSINESS_ROLE = \"Customer Service Representative\"\n",
        "#\n",
        "# BUSINESS_CONTEXT = \"Healthcare Organization\"\n",
        "# BUSINESS_ROLE = \"Medical Information Specialist\"\n",
        "\n",
        "print(f\"Business Context: {BUSINESS_CONTEXT}\")\n",
        "print(f\"Business Role: {BUSINESS_ROLE}\")\n",
        "print(\"\\nüí° This example will help you build an FAQ system for the IST402 course!\")\n",
        "print(\"   You can answer questions about assignments, technologies, and course content.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2: Design System Prompt\n",
        "\n",
        "Create a system prompt that defines the AI's role and context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Prompt Created:\n",
            "==================================================\n",
            "\n",
            "You are Week 3 Assignment FAQ Assistant and Concept Explainer for IST402 - AI Agents & RAG Systems Course.\n",
            "\n",
            "Your task is to create comprehensive question-answer pairs that would be useful for \n",
            "students working on the Week 3 RAG assignment. Focus on questions about:\n",
            "\n",
            "- Week 3 assignment requirements and instructions\n",
            "- RAG (Retrieval-Augmented Generation) concepts\n",
            "- FAISS vector database implementation\n",
            "- System prompts and their design\n",
            "- Mistral-7B-Instruct model usage\n",
            "- Embeddings and vector similarity search\n",
            "- How to complete specific assignment tasks\n",
            "- Troubleshooting common issues\n",
            "- Understanding key technologies (LangChain, FAISS, sentence-transformers)\n",
            "\n",
            "Guidelines:\n",
            "- Create clear, specific questions that students might ask\n",
            "- Provide accurate, detailed answers that help students learn\n",
            "- Cover different aspects: concepts, implementation, troubleshooting\n",
            "- Use clear, educational language that explains concepts well\n",
            "- Make answers practical and actionable for completing the assignment\n",
            "- Focus on Week 3 assignment-specific content\n",
            "\n",
            "Format each Q&A pair as:\n",
            "Q: [Question]\n",
            "A: [Answer]\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Design your system prompt\n",
        "# This prompt will be used to guide Mistral-7B in generating Q&A pairs\n",
        "\n",
        "# System prompt for IST402 Week 3 Assignment FAQ\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "You are {BUSINESS_ROLE} for {BUSINESS_CONTEXT}.\n",
        "\n",
        "Your task is to create comprehensive question-answer pairs that would be useful for \n",
        "students working on the Week 3 RAG assignment. Focus on questions about:\n",
        "\n",
        "- Week 3 assignment requirements and instructions\n",
        "- RAG (Retrieval-Augmented Generation) concepts\n",
        "- FAISS vector database implementation\n",
        "- System prompts and their design\n",
        "- Mistral-7B-Instruct model usage\n",
        "- Embeddings and vector similarity search\n",
        "- How to complete specific assignment tasks\n",
        "- Troubleshooting common issues\n",
        "- Understanding key technologies (LangChain, FAISS, sentence-transformers)\n",
        "\n",
        "Guidelines:\n",
        "- Create clear, specific questions that students might ask\n",
        "- Provide accurate, detailed answers that help students learn\n",
        "- Cover different aspects: concepts, implementation, troubleshooting\n",
        "- Use clear, educational language that explains concepts well\n",
        "- Make answers practical and actionable for completing the assignment\n",
        "- Focus on Week 3 assignment-specific content\n",
        "\n",
        "Format each Q&A pair as:\n",
        "Q: [Question]\n",
        "A: [Answer]\n",
        "\"\"\"\n",
        "\n",
        "print(\"System Prompt Created:\")\n",
        "print(\"=\" * 50)\n",
        "print(SYSTEM_PROMPT)\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Generate Business Database Content {#task-2}\n",
        "\n",
        "**Objective:** Use Mistral-7B-Instruct to generate 10-15 Q&A pairs for your business context.\n",
        "\n",
        "**Instructions:**\n",
        "- Use `mistralai/Mistral-7B-Instruct-v0.3`\n",
        "- Generate minimum 10-15 question-answer pairs\n",
        "- Cover different aspects of the business\n",
        "- **Add clear comments showing your generated Q&A pairs**\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1: Load Mistral-7B-Instruct Model\n",
        "\n",
        "**‚ö†Ô∏è CRITICAL: Before running this cell:**\n",
        "\n",
        "1. **Run the installation cell (Cell 2) above** to install all packages including `sentencepiece`\n",
        "2. **RESTART THE KERNEL** (Kernel ‚Üí Restart Kernel)\n",
        "3. **Then run this cell**\n",
        "\n",
        "**Why?** The Mistral tokenizer requires `sentencepiece`, and Python needs to reload after installation.\n",
        "\n",
        "**Note:** Loading the model may take several minutes on first run as it downloads ~14GB of model files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ sentencepiece is installed and ready\n",
            "\n",
            "Loading mistralai/Mistral-7B-Instruct-v0.3...\n",
            "This may take several minutes on first run (downloading ~14GB)...\n",
            "\n",
            "Step 1: Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer loaded successfully\n",
            "‚úÖ Padding token configured\n",
            "\n",
            "Step 2: Loading model (this may take a while)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b52bbd7991b342f88aa8b1a8020c92b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully (float16)\n",
            "\n",
            "============================================================\n",
            "‚úÖ mistralai/Mistral-7B-Instruct-v0.3 fully loaded and ready to use!\n",
            "============================================================\n",
            "Model device: cpu\n",
            "Model dtype: torch.float16\n",
            "Tokenizer vocab size: 32768\n"
          ]
        }
      ],
      "source": [
        "# Load Mistral-7B-Instruct model for generating Q&A pairs\n",
        "# Note: This may take a few minutes to download on first run\n",
        "\n",
        "# IMPORTANT: Check if sentencepiece is installed\n",
        "# If not installed, we'll try to install it automatically\n",
        "try:\n",
        "    import sentencepiece\n",
        "    print(\"‚úÖ sentencepiece is installed and ready\")\n",
        "except ImportError:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚ö†Ô∏è sentencepiece is NOT installed. Installing now...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Try to install using pip in the notebook\n",
        "    try:\n",
        "        # Use get_ipython() to run %pip install (works in Jupyter/Colab)\n",
        "        try:\n",
        "            get_ipython().run_line_magic('pip', 'install sentencepiece')\n",
        "            print(\"‚úÖ sentencepiece installed!\")\n",
        "        except:\n",
        "            # Fallback: use subprocess\n",
        "            import subprocess\n",
        "            import sys\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentencepiece\"])\n",
        "            print(\"‚úÖ sentencepiece installed!\")\n",
        "        \n",
        "        # Try importing again\n",
        "        try:\n",
        "            import sentencepiece\n",
        "            print(\"‚úÖ sentencepiece imported successfully!\")\n",
        "        except ImportError:\n",
        "            print(\"\\n‚ö†Ô∏è WARNING: sentencepiece was installed but cannot be imported yet.\")\n",
        "            print(\"   This usually means you need to RESTART THE KERNEL.\")\n",
        "            print(\"\\nüìã SOLUTION:\")\n",
        "            print(\"   1. RESTART THE KERNEL:\")\n",
        "            print(\"      - Jupyter: Kernel ‚Üí Restart Kernel\")\n",
        "            print(\"      - VS Code: Click 'Restart' in kernel toolbar\")\n",
        "            print(\"   2. Run this cell again\")\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            raise ImportError(\n",
        "                \"sentencepiece installed but requires kernel restart. \"\n",
        "                \"Please RESTART THE KERNEL and run this cell again.\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Failed to install sentencepiece automatically: {e}\")\n",
        "        print(\"\\nüìã MANUAL INSTALLATION:\")\n",
        "        print(\"   1. Run Cell 2 (Install Required Libraries) above\")\n",
        "        print(\"   2. RESTART THE KERNEL\")\n",
        "        print(\"   3. Run this cell again\")\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        raise ImportError(\n",
        "            \"Could not install sentencepiece. Please install manually using Cell 2, \"\n",
        "            \"then RESTART THE KERNEL.\"\n",
        "        )\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "print(f\"\\nLoading {model_name}...\")\n",
        "print(\"This may take several minutes on first run (downloading ~14GB)...\")\n",
        "\n",
        "# Load tokenizer - Mistral uses SentencePiece tokenizer\n",
        "print(\"\\nStep 1: Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"‚úÖ Tokenizer loaded successfully\")\n",
        "\n",
        "# Set padding token if not set (required for batch processing)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úÖ Padding token configured\")\n",
        "\n",
        "# Load model\n",
        "print(\"\\nStep 2: Loading model (this may take a while)...\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,  # Use float16 for faster inference and less memory\n",
        "        device_map=\"auto\",  # Automatically use GPU if available\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded successfully (float16)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error loading with float16: {e}\")\n",
        "    print(\"Trying with float32 (will use more memory)...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded successfully (float32)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ {model_name} fully loaded and ready to use!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2: Generate Q&A Pairs\n",
        "\n",
        "Generate 10-15 Q&A pairs using Mistral-7B-Instruct with your system prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "GENERATING Q&A PAIRS WITH MISTRAL-7B-INSTRUCT\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 15 Q&A pairs...\n",
            "Running on: CPU\n",
            "‚ö†Ô∏è Running on CPU - this will be slower. Consider using GPU for faster generation.\n",
            "This may take 2-5 minutes on CPU, or 30-60 seconds on GPU...\n"
          ]
        }
      ],
      "source": [
        "# Function to generate Q&A pairs using Mistral-7B-Instruct\n",
        "def generate_qa_pairs(prompt, num_pairs=15, model=None, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Generate Q&A pairs using Mistral-7B-Instruct\n",
        "    \n",
        "    Args:\n",
        "        prompt: System prompt with business context\n",
        "        num_pairs: Number of Q&A pairs to generate\n",
        "        model: The loaded Mistral model (uses global model if None)\n",
        "        tokenizer: The loaded tokenizer (uses global tokenizer if None)\n",
        "    \n",
        "    Returns:\n",
        "        List of (question, answer) tuples\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Use global model and tokenizer if not provided\n",
        "    if model is None:\n",
        "        model = globals().get('model')\n",
        "    if tokenizer is None:\n",
        "        tokenizer = globals().get('tokenizer')\n",
        "    \n",
        "    if model is None or tokenizer is None:\n",
        "        raise ValueError(\"Model and tokenizer must be loaded first. Run the model loading cell above.\")\n",
        "    \n",
        "    # Create the generation prompt\n",
        "    generation_prompt = f\"\"\"{prompt}\n",
        "\n",
        "Please generate exactly {num_pairs} question-answer pairs for this context.\n",
        "\n",
        "Format each pair as:\n",
        "Q: [Question]\n",
        "A: [Answer]\n",
        "\n",
        "Generate the Q&A pairs now:\"\"\"\n",
        "\n",
        "    # Format the conversation for Mistral using chat template\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Please generate exactly {num_pairs} question-answer pairs. Format each as:\\nQ: [Question]\\nA: [Answer]\"}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template and convert to tensors\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    \n",
        "    # Store input length to extract only new tokens later\n",
        "    input_length = inputs.shape[1]\n",
        "    \n",
        "    print(f\"Generating {num_pairs} Q&A pairs...\")\n",
        "    \n",
        "    # Performance note: Generation speed depends on:\n",
        "    # - Device (GPU is 10-50x faster than CPU)\n",
        "    # - Model size (Mistral-7B is large)\n",
        "    # - Number of tokens to generate\n",
        "    device_info = \"GPU\" if next(model.parameters()).is_cuda else \"CPU\"\n",
        "    print(f\"Running on: {device_info}\")\n",
        "    if device_info == \"CPU\":\n",
        "        print(\"‚ö†Ô∏è Running on CPU - this will be slower. Consider using GPU for faster generation.\")\n",
        "    print(\"This may take 2-5 minutes on CPU, or 30-60 seconds on GPU...\")\n",
        "    \n",
        "    # Calculate reasonable max tokens: ~80-100 tokens per Q&A pair\n",
        "    # This prevents generating too much unnecessary text\n",
        "    estimated_tokens = num_pairs * 100\n",
        "    max_tokens = min(estimated_tokens, 1500)  # Cap at 1500 to avoid excessive generation\n",
        "    \n",
        "    # Generate text\n",
        "    with torch.no_grad():  # Disable gradient computation for inference\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_tokens,  # Optimized: ~100 tokens per Q&A pair\n",
        "            temperature=0.7,  # Controls randomness (lower = more focused)\n",
        "            top_p=0.9,  # Nucleus sampling\n",
        "            do_sample=True,  # Enable sampling\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Extract only the newly generated tokens (skip the input tokens)\n",
        "    # outputs[0] contains the full sequence (input + generated), we only want the generated part\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    \n",
        "    # Decode only the newly generated text\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    print(\"‚úÖ Text generated, parsing Q&A pairs...\")\n",
        "    \n",
        "    # Parse Q&A pairs from the generated text\n",
        "    qa_pairs = []\n",
        "    \n",
        "    # Pattern to match Q: ... A: ... format\n",
        "    pattern = r'Q:\\s*(.+?)(?=\\nA:|\\nQ:|$)'\n",
        "    answer_pattern = r'A:\\s*(.+?)(?=\\nQ:|$)'\n",
        "    \n",
        "    # Split by Q: markers\n",
        "    qa_blocks = re.split(r'\\n\\s*Q:\\s*', generated_text, flags=re.IGNORECASE)\n",
        "    \n",
        "    for block in qa_blocks:\n",
        "        if not block.strip():\n",
        "            continue\n",
        "            \n",
        "        # Extract question (first line or until A:)\n",
        "        question_match = re.match(r'^(.+?)(?=\\n\\s*A:|\\nQ:|$)', block, re.DOTALL)\n",
        "        if question_match:\n",
        "            question = question_match.group(1).strip()\n",
        "            \n",
        "            # Extract answer (after A:)\n",
        "            answer_match = re.search(r'\\n\\s*A:\\s*(.+?)(?=\\n\\s*Q:|$)', block, re.DOTALL)\n",
        "            if answer_match:\n",
        "                answer = answer_match.group(1).strip()\n",
        "                \n",
        "                # Clean up the question and answer\n",
        "                question = question.strip().strip('Q:').strip()\n",
        "                answer = answer.strip().strip('A:').strip()\n",
        "                \n",
        "                if question and answer and len(question) > 5 and len(answer) > 10:\n",
        "                    qa_pairs.append((question, answer))\n",
        "    \n",
        "    # If regex parsing didn't work well, try simpler approach\n",
        "    if len(qa_pairs) < num_pairs // 2:\n",
        "        print(\"‚ö†Ô∏è Regex parsing found fewer pairs. Trying alternative parsing...\")\n",
        "        # Alternative: split by lines and look for Q: and A: patterns\n",
        "        lines = generated_text.split('\\n')\n",
        "        current_q = None\n",
        "        current_a = None\n",
        "        \n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('Q:') or line.startswith('q:'):\n",
        "                if current_q and current_a:\n",
        "                    qa_pairs.append((current_q, current_a))\n",
        "                current_q = line.replace('Q:', '').replace('q:', '').strip()\n",
        "                current_a = None\n",
        "            elif line.startswith('A:') or line.startswith('a:'):\n",
        "                current_a = line.replace('A:', '').replace('a:', '').strip()\n",
        "            elif current_a:\n",
        "                current_a += ' ' + line\n",
        "            elif current_q and not current_a:\n",
        "                current_q += ' ' + line\n",
        "        \n",
        "        if current_q and current_a:\n",
        "            qa_pairs.append((current_q, current_a))\n",
        "    \n",
        "    print(f\"‚úÖ Parsed {len(qa_pairs)} Q&A pairs from generated text\")\n",
        "    \n",
        "    # If we still don't have enough, generate more\n",
        "    if len(qa_pairs) < num_pairs:\n",
        "        print(f\"‚ö†Ô∏è Only found {len(qa_pairs)} pairs, need {num_pairs}. Generating additional pairs...\")\n",
        "        # Could call recursively or generate more, but for now return what we have\n",
        "        # In practice, you might want to adjust the prompt or generate in batches\n",
        "    \n",
        "    return qa_pairs[:num_pairs]  # Return up to num_pairs\n",
        "\n",
        "# Generate Q&A pairs using the loaded model and tokenizer\n",
        "# Make sure you've run the model loading cell (Cell 10) first!\n",
        "print(\"=\" * 70)\n",
        "print(\"GENERATING Q&A PAIRS WITH MISTRAL-7B-INSTRUCT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "qa_pairs = generate_qa_pairs(SYSTEM_PROMPT, num_pairs=15)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully generated {len(qa_pairs)} Q&A pairs\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GENERATED Q&A PAIRS:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, (q, a) in enumerate(qa_pairs, 1):\n",
        "    print(f\"\\n{i}. Q: {q}\")\n",
        "    print(f\"   A: {a}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3: Display All Generated Q&A Pairs\n",
        "\n",
        "This section displays all the Q&A pairs that were generated using Mistral-7B-Instruct in the previous step. These pairs will be used to build your knowledge base for the RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all generated Q&A pairs from the previous cell\n",
        "# The qa_pairs variable was created in Cell 12 (Task 2.2)\n",
        "\n",
        "# Check if qa_pairs exists (from previous cell)\n",
        "if 'qa_pairs' not in globals() or not qa_pairs:\n",
        "    print(\"‚ö†Ô∏è WARNING: No Q&A pairs found!\")\n",
        "    print(\"Please run Cell 12 (Task 2.2) first to generate Q&A pairs.\")\n",
        "    print(\"Creating empty list for now...\")\n",
        "    faq_data = []\n",
        "else:\n",
        "    # Use the generated Q&A pairs\n",
        "    faq_data = qa_pairs.copy()\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"GENERATED Q&A DATABASE FOR {BUSINESS_CONTEXT}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n‚úÖ Q&A Database created with {len(faq_data)} pairs\")\n",
        "    print(f\"Business Role: {BUSINESS_ROLE}\")\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ALL GENERATED Q&A PAIRS:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Display all Q&A pairs in a clear format\n",
        "    for i, (q, a) in enumerate(faq_data, 1):\n",
        "        print(f\"\\n{i}. Q: {q}\")\n",
        "        print(f\"   A: {a}\")\n",
        "        print(\"-\" * 70)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Total: {len(faq_data)} Q&A pairs ready for vector database\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Implement FAISS Vector Database {#task-3}\n",
        "\n",
        "**Objective:** Convert Q&A pairs into embeddings and store in FAISS index.\n",
        "\n",
        "**Instructions:**\n",
        "- Convert Q&A pairs to embeddings\n",
        "- Store in FAISS index\n",
        "- **Use comments to demonstrate the implementation process**\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1: Convert Q&A Pairs to LangChain Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Q&A pairs into LangChain Document objects\n",
        "# Each document contains both question and answer as searchable content\n",
        "\n",
        "# Combine question and answer for each pair to create comprehensive documents\n",
        "documents = [Document(page_content=qa[0] + \" \" + qa[1]) for qa in faq_data]\n",
        "\n",
        "print(f\"‚úÖ Created {len(documents)} LangChain documents\")\n",
        "print(f\"\\nSample document:\")\n",
        "print(f\"Content: {documents[0].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2: Create Embeddings Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings model to convert text into numerical vectors\n",
        "# We use a pre-trained model that's good at understanding sentence meanings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"‚úÖ Embeddings model loaded\")\n",
        "print(f\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(f\"Vector dimensions: 384\")\n",
        "\n",
        "# Optional: Test embedding generation\n",
        "sample_text = \"What is your return policy?\"\n",
        "sample_embedding = embeddings.embed_query(sample_text)\n",
        "print(f\"\\nSample embedding shape: {len(sample_embedding)} dimensions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3: Build FAISS Vector Database\n",
        "\n",
        "**Implementation Process:**\n",
        "1. Convert all documents to embeddings\n",
        "2. Create FAISS index for efficient similarity search\n",
        "3. Store the index for fast retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build FAISS vector database from documents\n",
        "# This creates an optimized index for fast similarity search\n",
        "\n",
        "# Step 1: Convert documents to embeddings and create FAISS index\n",
        "db = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "print(\"‚úÖ FAISS vector database created successfully!\")\n",
        "print(f\"Number of documents indexed: {len(documents)}\")\n",
        "print(f\"Index type: FAISS\")\n",
        "\n",
        "# Test similarity search\n",
        "test_query = \"What is your return policy?\"\n",
        "test_results = db.similarity_search(test_query, k=2)\n",
        "print(f\"\\nTest query: '{test_query}'\")\n",
        "print(f\"Retrieved {len(test_results)} similar documents\")\n",
        "print(f\"\\nMost similar document:\")\n",
        "print(f\"Content: {test_results[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Create Test Questions {#task-4}\n",
        "\n",
        "**Objective:** Generate two types of questions using Mistral-7B-Instruct:\n",
        "- **Answerable questions** (5+): Can be answered from your database\n",
        "- **Unanswerable questions** (5+): Require information not in your database\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1: Generate Answerable Questions\n",
        "\n",
        "Questions that can be directly answered from your Q&A database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generate 5+ answerable questions using Mistral-7B-Instruct\n",
        "# These questions should be answerable from your Q&A database\n",
        "\n",
        "# Example: Use Mistral to generate questions based on your database topics\n",
        "answerable_questions = [\n",
        "    # TODO: Add your generated answerable questions here\n",
        "    # Example: \"What is your return policy?\",\n",
        "    # Example: \"Do you ship internationally?\",\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Generated {len(answerable_questions)} answerable questions\")\n",
        "print(\"\\nAnswerable Questions:\")\n",
        "for i, q in enumerate(answerable_questions, 1):\n",
        "    print(f\"{i}. {q}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generate 5+ unanswerable questions using Mistral-7B-Instruct\n",
        "# These questions should NOT be answerable from your Q&A database\n",
        "# They test whether the system correctly identifies its limitations\n",
        "\n",
        "unanswerable_questions = [\n",
        "    # TODO: Add your generated unanswerable questions here\n",
        "    # Example: \"What is your company's stock price?\",\n",
        "    # Example: \"Do you offer services in Antarctica?\",\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Generated {len(unanswerable_questions)} unanswerable questions\")\n",
        "print(\"\\nUnanswerable Questions:\")\n",
        "for i, q in enumerate(unanswerable_questions, 1):\n",
        "    print(f\"{i}. {q}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Implement and Test Questions {#task-5}\n",
        "\n",
        "**Objective:** Run both question types through your RAG system and analyze results.\n",
        "\n",
        "**Instructions:**\n",
        "- Test answerable questions (should get good answers)\n",
        "- Test unanswerable questions (should get \"I don't know\" or low confidence)\n",
        "- **Use clear comments to differentiate between question types**\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1: Load QA Model\n",
        "\n",
        "Load a question-answering model to test the RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained question-answering model\n",
        "# We'll start with DistilBERT as a baseline, then test other models in Task 6\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "print(\"‚úÖ QA model loaded successfully!\")\n",
        "print(\"Model: distilbert-base-uncased-distilled-squad\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2: Implement RAG Pipeline Function\n",
        "\n",
        "Create a function that implements the complete RAG pipeline:\n",
        "1. Retrieve relevant context from FAISS\n",
        "2. Augment query with context\n",
        "3. Generate answer using QA model\n",
        "4. Apply confidence threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_qa_system(question, db, qa_pipeline, k=2, confidence_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline for question answering\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        db: FAISS vector database\n",
        "        qa_pipeline: Question-answering model pipeline\n",
        "        k: Number of documents to retrieve\n",
        "        confidence_threshold: Minimum confidence score for accepting answer\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'answer', 'confidence', 'context_retrieved', 'is_answerable'\n",
        "    \"\"\"\n",
        "    # STEP 1: RETRIEVE - Find relevant documents from FAISS\n",
        "    docs = db.similarity_search(question, k=k)\n",
        "    \n",
        "    # STEP 2: AUGMENT - Combine retrieved documents into context\n",
        "    context = \" \".join([d.page_content for d in docs])\n",
        "    \n",
        "    # STEP 3: GENERATE - Use QA model to generate answer\n",
        "    result = qa_pipeline({\"question\": question, \"context\": context})\n",
        "    \n",
        "    # STEP 4: EVALUATE - Check confidence and apply threshold\n",
        "    answer = result[\"answer\"] if result.get(\"score\", 0) > confidence_threshold else \"I don't know.\"\n",
        "    confidence = result.get(\"score\", 0)\n",
        "    \n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": confidence,\n",
        "        \"context_retrieved\": context[:200] + \"...\" if len(context) > 200 else context,\n",
        "        \"is_answerable\": confidence > confidence_threshold\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ RAG pipeline function created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3: Test Answerable Questions\n",
        "\n",
        "**Expected Result:** System should provide accurate answers with good confidence scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test answerable questions\n",
        "# These should retrieve relevant context and provide good answers\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING ANSWERABLE QUESTIONS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Expected: Good answers with high confidence scores\\n\")\n",
        "\n",
        "for i, question in enumerate(answerable_questions, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    result = rag_qa_system(question, db, qa_pipeline)\n",
        "    \n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Retrieved Context: {result['context_retrieved']}\")\n",
        "    print(f\"Status: {'‚úÖ Answerable' if result['is_answerable'] else '‚ùå Low Confidence'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4: Test Unanswerable Questions\n",
        "\n",
        "**Expected Result:** System should identify limitations and respond with \"I don't know\" or low confidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test unanswerable questions\n",
        "# These should NOT find relevant context and should respond appropriately\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING UNANSWERABLE QUESTIONS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Expected: 'I don't know' or low confidence scores\\n\")\n",
        "\n",
        "for i, question in enumerate(unanswerable_questions, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    result = rag_qa_system(question, db, qa_pipeline)\n",
        "    \n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Retrieved Context: {result['context_retrieved']}\")\n",
        "    print(f\"Status: {'‚ö†Ô∏è Attempted Answer (Low Confidence)' if not result['is_answerable'] else '‚ùå Should be unanswerable'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6: Model Experimentation & Ranking {#task-6}\n",
        "\n",
        "**Objective:** Test 6 different QA models and rank them by performance.\n",
        "\n",
        "**Required Models:**\n",
        "1. `consciousAI/question-answering-generative-t5-v1-base-s-q-c`\n",
        "2. `deepset/roberta-base-squad2`\n",
        "3. `google-bert/bert-large-cased-whole-word-masking-finetuned-squad`\n",
        "4. `gasolsun/DynamicRAG-8B`\n",
        "5. **[Your Choice 1]**\n",
        "6. **[Your Choice 2]**\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "- Accuracy on answerable questions\n",
        "- Appropriate handling of unanswerable questions\n",
        "- Response quality\n",
        "- Speed (latency)\n",
        "- Robustness\n",
        "\n",
        "---\n",
        "\n",
        "### 6.1: Define Models to Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define all models to test\n",
        "models_to_test = {\n",
        "    \"Model 1 - T5 Generative\": \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\",\n",
        "    \"Model 2 - RoBERTa\": \"deepset/roberta-base-squad2\",\n",
        "    \"Model 3 - BERT Large\": \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\",\n",
        "    \"Model 4 - DynamicRAG\": \"gasolsun/DynamicRAG-8B\",\n",
        "    \"Model 5 - [Your Choice 1]\": \"[YOUR_MODEL_1_HERE]\",  # TODO: Replace with your choice\n",
        "    \"Model 6 - [Your Choice 2]\": \"[YOUR_MODEL_2_HERE]\",  # TODO: Replace with your choice\n",
        "}\n",
        "\n",
        "print(\"Models to test:\")\n",
        "for name, model in models_to_test.items():\n",
        "    print(f\"  - {name}: {model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2: Test Each Model\n",
        "\n",
        "Test all models on both answerable and unanswerable questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Store results for all models\n",
        "all_results = []\n",
        "\n",
        "# Test each model\n",
        "for model_name, model_path in models_to_test.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Testing: {model_name}\")\n",
        "    print(f\"Model: {model_path}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    try:\n",
        "        # Load model\n",
        "        qa_pipeline = pipeline(\"question-answering\", model=model_path)\n",
        "        \n",
        "        # Test on answerable questions\n",
        "        answerable_results = []\n",
        "        for q in answerable_questions:\n",
        "            start_time = time.time()\n",
        "            result = rag_qa_system(q, db, qa_pipeline)\n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            answerable_results.append({\n",
        "                \"question\": q,\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"confidence\": result[\"confidence\"],\n",
        "                \"time\": elapsed,\n",
        "                \"type\": \"answerable\"\n",
        "            })\n",
        "        \n",
        "        # Test on unanswerable questions\n",
        "        unanswerable_results = []\n",
        "        for q in unanswerable_questions:\n",
        "            start_time = time.time()\n",
        "            result = rag_qa_system(q, db, qa_pipeline)\n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            unanswerable_results.append({\n",
        "                \"question\": q,\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"confidence\": result[\"confidence\"],\n",
        "                \"time\": elapsed,\n",
        "                \"type\": \"unanswerable\"\n",
        "            })\n",
        "        \n",
        "        # Calculate metrics\n",
        "        avg_confidence_answerable = sum(r[\"confidence\"] for r in answerable_results) / len(answerable_results)\n",
        "        avg_confidence_unanswerable = sum(r[\"confidence\"] for r in unanswerable_results) / len(unanswerable_results)\n",
        "        avg_time = sum(r[\"time\"] for r in answerable_results + unanswerable_results) / (len(answerable_results) + len(unanswerable_results))\n",
        "        \n",
        "        all_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"model_path\": model_path,\n",
        "            \"avg_confidence_answerable\": avg_confidence_answerable,\n",
        "            \"avg_confidence_unanswerable\": avg_confidence_unanswerable,\n",
        "            \"avg_time\": avg_time,\n",
        "            \"answerable_results\": answerable_results,\n",
        "            \"unanswerable_results\": unanswerable_results\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Completed testing {model_name}\")\n",
        "        print(f\"   Avg Confidence (Answerable): {avg_confidence_answerable:.3f}\")\n",
        "        print(f\"   Avg Confidence (Unanswerable): {avg_confidence_unanswerable:.3f}\")\n",
        "        print(f\"   Avg Time: {avg_time:.3f}s\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing {model_name}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n‚úÖ Completed testing {len(all_results)} models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for result in all_results:\n",
        "    comparison_data.append({\n",
        "        \"Model\": result[\"model_name\"],\n",
        "        \"Avg Confidence (Answerable)\": result[\"avg_confidence_answerable\"],\n",
        "        \"Avg Confidence (Unanswerable)\": result[\"avg_confidence_unanswerable\"],\n",
        "        \"Avg Time (seconds)\": result[\"avg_time\"],\n",
        "        \"Confidence Gap\": result[\"avg_confidence_answerable\"] - result[\"avg_confidence_unanswerable\"]\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Sort by overall performance (you can adjust sorting criteria)\n",
        "df_comparison = df_comparison.sort_values(\"Avg Confidence (Answerable)\", ascending=False)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL COMPARISON TABLE\")\n",
        "print(\"=\" * 70)\n",
        "print(df_comparison.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4: Rank Models and Provide Justification\n",
        "\n",
        "Rank models from best to worst and explain your reasoning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Rank models and provide justification\n",
        "# Consider: Accuracy, Speed, Confidence Handling, Response Quality, Robustness\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL RANKING (Best to Worst)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example ranking structure (customize based on your results)\n",
        "rankings = [\n",
        "    {\n",
        "        \"Rank\": 1,\n",
        "        \"Model\": \"[Best Model Name]\",\n",
        "        \"Justification\": \"[Explain why this model performed best]\"\n",
        "    },\n",
        "    # TODO: Add rankings for all 6 models\n",
        "]\n",
        "\n",
        "for ranking in rankings:\n",
        "    print(f\"\\n{ranking['Rank']}. {ranking['Model']}\")\n",
        "    print(f\"   Justification: {ranking['Justification']}\")\n",
        "\n",
        "# Display detailed analysis\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DETAILED ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# TODO: Add your detailed analysis here\n",
        "# - Which models provide confidence scores?\n",
        "# - Which models handle unanswerable questions best?\n",
        "# - Speed vs. accuracy trade-offs\n",
        "# - Recommendations for different use cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection & Analysis {#reflection}\n",
        "\n",
        "**Objective:** Reflect on the assignment, analyze strengths/weaknesses, and discuss real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "### Reflection Questions\n",
        "\n",
        "1. **What worked well?**\n",
        "2. **What were the main challenges?**\n",
        "3. **How could the system be improved?**\n",
        "4. **What are the real-world applications?**\n",
        "5. **What did you learn?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strengths of the System\n",
        "\n",
        "**TODO:** Document the strengths of your RAG system implementation.\n",
        "\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weaknesses and Limitations\n",
        "\n",
        "**TODO:** Document the weaknesses and limitations you identified.\n",
        "\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "**TODO:** Discuss how this RAG system could be used in real-world scenarios.\n",
        "\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Learnings\n",
        "\n",
        "**TODO:** Summarize what you learned from this assignment.\n",
        "\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Assignment Complete! ‚úÖ\n",
        "\n",
        "**Submission Checklist:**\n",
        "- [ ] All 6 tasks completed\n",
        "- [ ] 10-15 Q&A pairs generated and documented\n",
        "- [ ] FAISS vector database implemented\n",
        "- [ ] 5+ answerable and 5+ unanswerable questions created\n",
        "- [ ] All 6 models tested and compared\n",
        "- [ ] Models ranked with justifications\n",
        "- [ ] Reflection completed\n",
        "- [ ] Code is well-commented\n",
        "- [ ] Notebook is well-formatted and organized\n",
        "\n",
        "**Next Steps:**\n",
        "1. Review your notebook for completeness\n",
        "2. Ensure all code runs without errors\n",
        "3. Add any additional analysis or insights\n",
        "4. Submit the link to your completed notebook\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your submission!** üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2: Generate Unanswerable Questions\n",
        "\n",
        "Questions that require information NOT present in your database.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
