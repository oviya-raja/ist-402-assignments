{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ğŸ“š PDF Q&A RAG â€” Launcher\n",
        "# =========================\n",
        "\n",
        "# 1) Install deps\n",
        "!pip -q install streamlit langchain-community faiss-cpu sentence-transformers \\\n",
        "                transformers accelerate safetensors pypdf > /dev/null\n",
        "\n",
        "# 2) Write the Streamlit app\n",
        "app_code = r\"\"\"\n",
        "import os\n",
        "import io\n",
        "import torch\n",
        "import streamlit as st\n",
        "\n",
        "# ---- LangChain & friends ----\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ---- PDF parsing ----\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# ---- Local LLM (FLAN-T5) ----\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "st.set_page_config(page_title=\"PDF Q&A (Local RAG)\", page_icon=\"ğŸ“š\")\n",
        "st.title(\" PDF Q&A Chatbot â€” Local RAG (LangChain + FLAN-T5)\")\n",
        "\n",
        "st.markdown(\n",
        "    \"Upload one or more PDFs. Weâ€™ll chunk + embed them (MiniLM), build a FAISS index, \"\n",
        "    \"then answer questions using retrieved chunks and a local FLAN-T5 model.\"\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# CACHED MODELS\n",
        "# ---------------------------\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_flan():\n",
        "    name = \"google/flan-t5-base\"\n",
        "    tok = AutoTokenizer.from_pretrained(name)\n",
        "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(name, torch_dtype=dtype)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    return tok, model, device\n",
        "\n",
        "embeddings = load_embeddings()\n",
        "tokenizer, flan, device = load_flan()\n",
        "\n",
        "# ---------------------------\n",
        "# HELPERS\n",
        "# ---------------------------\n",
        "def read_pdfs(files):\n",
        "    texts = []\n",
        "    for f in files:\n",
        "        data = f.read()\n",
        "        reader = PdfReader(io.BytesIO(data))\n",
        "        content = []\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                content.append(page.extract_text() or \"\")\n",
        "            except Exception:\n",
        "                content.append(\"\")\n",
        "        full_text = \"\\\\n\".join(content).strip()\n",
        "        if full_text:\n",
        "            texts.append(full_text)\n",
        "    return texts\n",
        "\n",
        "def build_vectorstore(raw_texts):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=150,\n",
        "        length_function=len,\n",
        "    )\n",
        "    docs = []\n",
        "    for t in raw_texts:\n",
        "        docs.extend(splitter.create_documents([t]))\n",
        "    vs = FAISS.from_documents(docs, embeddings)\n",
        "    return vs\n",
        "\n",
        "def make_prompt(question, contexts):\n",
        "    context_block = \"\\\\n\\\\n\".join([f\"[Chunk {i+1}]\\\\n{c}\" for i, c in enumerate(contexts)])\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. Answer the question using ONLY the context.\\\\n\"\n",
        "        \"If the answer is not in the context, say you don't know.\\\\n\\\\n\"\n",
        "        f\"Context:\\\\n{context_block}\\\\n\\\\n\"\n",
        "        f\"Question: {question}\\\\nAnswer:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def generate_answer(prompt, max_new_tokens=256):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = flan.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.2,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------------------\n",
        "# UI\n",
        "# ---------------------------\n",
        "st.subheader(\"ğŸ“¤ Upload PDFs\")\n",
        "uploaded = st.file_uploader(\"Upload one or more PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "if 'vectorstore' not in st.session_state:\n",
        "    st.session_state.vectorstore = None\n",
        "\n",
        "col_a, col_b = st.columns([1,1])\n",
        "with col_a:\n",
        "    build_btn = st.button(\"ğŸ”§ Build / Rebuild Index\")\n",
        "with col_b:\n",
        "    clear_btn = st.button(\"ğŸ—‘ï¸ Clear Index\")\n",
        "\n",
        "if clear_btn:\n",
        "    st.session_state.vectorstore = None\n",
        "    st.success(\"Cleared vector index.\")\n",
        "\n",
        "if build_btn:\n",
        "    if not uploaded:\n",
        "        st.warning(\"Please upload at least one PDF.\")\n",
        "    else:\n",
        "        with st.spinner(\"Reading PDFs and building FAISS index...\"):\n",
        "            texts = read_pdfs(uploaded)\n",
        "            if not any(texts):\n",
        "                st.error(\"No extractable text found in the PDFs.\")\n",
        "            else:\n",
        "                st.session_state.vectorstore = build_vectorstore(texts)\n",
        "                st.success(\"Index ready! Ask questions below.\")\n",
        "\n",
        "st.divider()\n",
        "st.subheader(\" Ask a Question\")\n",
        "\n",
        "q = st.text_input(\"Your question\")\n",
        "k = st.slider(\"Top-k chunks\", 2, 8, 4)\n",
        "max_tokens = st.slider(\"Max new tokens (answer length)\", 64, 512, 256, step=32)\n",
        "\n",
        "if st.session_state.vectorstore is None:\n",
        "    st.info(\" Upload PDFs and click **Build / Rebuild Index** to start.\")\n",
        "else:\n",
        "    if st.button(\" Retrieve & Answer\", disabled=(not q.strip())):\n",
        "        with st.spinner(\"Retrieving relevant chunks...\"):\n",
        "            docs = st.session_state.vectorstore.similarity_search(q, k=k)\n",
        "            contexts = [d.page_content for d in docs]\n",
        "        st.write(\"**Retrieved Chunks:**\")\n",
        "        for i, c in enumerate(contexts, 1):\n",
        "            with st.expander(f\"Chunk {i}\"):\n",
        "                st.write(c)\n",
        "\n",
        "        with st.spinner(\"Generating answer with FLAN-T5...\"):\n",
        "            prompt = make_prompt(q, contexts)\n",
        "            ans = generate_answer(prompt, max_new_tokens=max_tokens)\n",
        "        st.success(\"**Answer:**\")\n",
        "        st.write(ans)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# 3) Kill anything on 8501 and start Streamlit\n",
        "!pkill -f streamlit || true\n",
        "!fuser -k 8501/tcp || true\n",
        "!streamlit run app.py --server.port 8501 --server.headless true &>/dev/null&\n",
        "\n",
        "# 4) Open a Cloudflare tunnel (no account required) and print the URL\n",
        "import subprocess, time, re, sys\n",
        "\n",
        "# fetch latest cloudflared\n",
        "!wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared\n",
        "\n",
        "p = subprocess.Popen(\n",
        "    [\"./cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "url = None\n",
        "for _ in range(300):  # ~30s\n",
        "    line = p.stdout.readline()\n",
        "    if not line:\n",
        "        time.sleep(0.1); continue\n",
        "    if \"trycloudflare.com\" in line:\n",
        "        for token in line.split():\n",
        "            if token.startswith(\"https://\") and \"trycloudflare.com\" in token:\n",
        "                url = token.strip()\n",
        "                break\n",
        "    if url:\n",
        "        break\n",
        "\n",
        "print(\" Streamlit app URL:\", url if url else \"Still starting... check the logs above.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EDxlIN52Tzl",
        "outputId": "c5fceadc-bbb3-4418-af26-2d2360989eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "âœ… Streamlit app URL: https://stands-happens-devoted-vitamin.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit langchain-community faiss-cpu sentence-transformers \\\n",
        "            transformers accelerate safetensors pypdf -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YGeLDme2U1_",
        "outputId": "348529a9-c1b2-41d0-d7e9-6bd811a8643e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m725.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}